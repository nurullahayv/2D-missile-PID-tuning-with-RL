# ğŸ Bee Colony Honeycomb Construction Simulation

Bu repository, **Hierarchical Cooperative Multi-Agent Reinforcement Learning (MARL)** kullanarak arÄ± kolonisinin petek inÅŸa etme davranÄ±ÅŸÄ±nÄ± simÃ¼le etmek iÃ§in gÃ¼ncellenmiÅŸtir.

## ğŸ“‹ Ä°Ã§indekiler

- [Genel BakÄ±ÅŸ](#genel-bakÄ±ÅŸ)
- [Sistem Mimarisi](#sistem-mimarisi)
- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [KonfigÃ¼rasyon](#konfigÃ¼rasyon)
- [Dosya YapÄ±sÄ±](#dosya-yapÄ±sÄ±)

---

## ğŸ¯ Genel BakÄ±ÅŸ

### Temel Ã–zellikler

- **Hierarchical MARL**: Ä°ki seviyeli politika yapÄ±sÄ±
  - **High-Level Policy**: Her arÄ± hangi gÃ¶rev/bÃ¶lgeye odaklanacaÄŸÄ±na karar verir
  - **Low-Level Policy**: Hareket ve inÅŸa eylemlerini gerÃ§ekleÅŸtirir

- **Cooperative Learning**: ArÄ±lar iÅŸbirliÄŸi yaparak petek inÅŸa eder
  - Birlikte inÅŸa edildiÄŸinde sÃ¼re kÄ±salÄ±r (256 tick â†’ 128 â†’ 64 â†’ ... â†’ 1 tick)
  - BitiÅŸik duvarlar iÃ§in koordinasyon bonusu
  - KapalÄ± alan (enclosed area) oluÅŸturma iÃ§in ortak Ã¶dÃ¼l

- **Grid-Based World**: 500x500 grid dÃ¼nyasÄ±
  - 32 yÃ¶nlÃ¼ smooth hareket
  - 8x8 local observation window
  - Decaying memory (ziyaret edilen alanlar)

- **Building Mechanics**:
  - ArÄ±lar bulunduklarÄ± pozisyonun 8 komÅŸusuna duvar inÅŸa edebilir
  - Collaborative building: Birden fazla arÄ± aynÄ± yere inÅŸa ederse hÄ±zlanÄ±r
  - Redundant building penalty: TamamlanmÄ±ÅŸ duvara tekrar inÅŸa etmek ceza

- **Reward System**:
  - Enclosed area increase: KapalÄ± alanlar bÃ¼yÃ¼dÃ¼kÃ§e Ã¶dÃ¼l
  - Adjacent wall bonus: BitiÅŸik duvar inÅŸa etme bonusu
  - Coordination bonus: Ä°ÅŸbirlikli inÅŸa bonusu
  - Penalties: Gereksiz/tekrarlÄ± inÅŸa cezasÄ±

---

## ğŸ—ï¸ Sistem Mimarisi

### Hierarchical Policy Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         High-Level Policy (Her arÄ± iÃ§in)â”‚
â”‚  GÃ¶rev SeÃ§imi:                          â”‚
â”‚  - Explore freely                       â”‚
â”‚  - Build zone NW                        â”‚
â”‚  - Build zone NE                        â”‚
â”‚  - Build zone S                         â”‚
â”‚  (10-20 step'te bir Ã§alÄ±ÅŸÄ±r)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Low-Level Policy (Her step)     â”‚
â”‚  Eylemler:                              â”‚
â”‚  - Direction: 32 yÃ¶n (0-31)             â”‚
â”‚  - Build: 9 seÃ§enek (0=yok, 1-8=komÅŸu) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Observation Space

#### Low-Level Observation
```python
{
    'grid_obs': (8, 8, 4),  # Local 8x8 window
        # Channel 0: Other bees (1.0=idle, 0.5=building)
        # Channel 1: Walls (1.0=wall exists)
        # Channel 2: Visited areas (decaying memory)
        # Channel 3: Build progress (0 to 1)

    'scalar_obs': [x, y, direction, current_task]  # Normalized
}
```

#### High-Level Observation
```python
{
    'global_obs': (16, 16, 3),  # Global downsampled view
        # Channel 0: Bee density
        # Channel 1: Wall density
        # Channel 2: Build activity

    'scalar_obs': [x, y, direction, prev_task]  # Normalized
}
```

### Neural Network Architecture

#### Low-Level Model
```
Input: 8x8x4 grid + 4 scalar features
  â†“
CNN: Conv2D(4â†’16) â†’ Conv2D(16â†’32) â†’ Conv2D(32â†’64)
  â†“
Flatten + Concat with scalar
  â†“
FC: 1024+4 â†’ 256 â†’ 256
  â†“
Output: 32 (direction) + 9 (build) logits

Centralized Critic:
  Input: All bees' observations (7 Ã— (1024+4))
  â†“
  FC: 7168 â†’ 512 â†’ 256 â†’ 1 (value)
```

#### High-Level Model
```
Input: 16x16x3 global + 4 scalar features
  â†“
CNN: Conv2D(3â†’16) â†’ Conv2D(16â†’32) â†’ Conv2D(32â†’64)
  â†“
Flatten + Concat with scalar
  â†“
FC: 1024+4 â†’ 256 â†’ 128
  â†“
Output: 4 task logits

Centralized Critic: Similar structure
```

---

## ğŸš€ Kurulum

### 1. Gerekli Paketleri YÃ¼kleyin

```bash
pip install -r requirements_bee_colony.txt
```

Veya manuel olarak:

```bash
pip install numpy torch gymnasium ray[rllib] matplotlib tqdm tensorboard
```

### 2. Test Edin

```bash
python test_bee_colony.py
```

Bu test ÅŸunlarÄ± kontrol eder:
- âœ“ Simulator Ã§alÄ±ÅŸÄ±yor mu
- âœ“ Environment doÄŸru Ã§alÄ±ÅŸÄ±yor mu
- âœ“ Enclosed area hesaplamasÄ± doÄŸru mu
- âœ“ Visualization Ã§alÄ±ÅŸÄ±yor mu

---

## ğŸ® KullanÄ±m

### 1. Low-Level Policy Training

Ä°lk olarak low-level policy'yi eÄŸitin (hareket + inÅŸa):

```bash
python train_bee_lowlevel.py --level 1 --epochs 2000
```

**Parametreler:**
- `--level`: Training level (1-5, curriculum learning)
- `--epochs`: Training epoch sayÄ±sÄ±
- `--num_bees`: ArÄ± sayÄ±sÄ± (default: 7)
- `--grid_size`: Grid boyutu (default: 500)
- `--horizon`: Episode uzunluÄŸu (default: 5000)
- `--batch_size`: PPO batch size (default: 4000)
- `--lr`: Learning rate (default: 5e-5)
- `--gpu`: GPU kullanÄ±mÄ± (default: 0)
- `--num_workers`: Parallel worker sayÄ±sÄ± (default: 4)

**Checkpoint:**
EÄŸitim sonucu `results/BeeColony_LowLevel_L1_7bees/checkpoint/` altÄ±na kaydedilir.

### 2. High-Level Policy Training

Low-level policy eÄŸitildikten sonra, high-level policy'yi eÄŸitin:

```bash
python train_bee_highlevel.py --epochs 1000
```

High-level policy otomatik olarak en son low-level checkpoint'i yÃ¼kler.

### 3. Evaluation

EÄŸitilmiÅŸ policy'leri deÄŸerlendirin:

```bash
python evaluate_bee_colony.py --checkpoint results/BeeColony_HighLevel_7bees/checkpoint
```

---

## âš™ï¸ KonfigÃ¼rasyon

### BeeColonyConfig (config.py)

```python
from config import BeeColonyConfig

# Mode 0: Low-level training
config = BeeColonyConfig(mode=0)

# Mode 1: High-level training
config = BeeColonyConfig(mode=1)

# Mode 2: Evaluation
config = BeeColonyConfig(mode=2)
```

### Ã–nemli Parametreler

#### Environment Parameters
```python
--grid_size 500           # 500x500 grid dÃ¼nyasÄ±
--num_bees 7              # 7 arÄ±
--num_directions 32       # 32 yÃ¶nlÃ¼ hareket
--window_size 8           # 8x8 local observation
--movement_speed 1.0      # Hareket hÄ±zÄ±
--base_build_ticks 256    # Tek arÄ± iÃ§in inÅŸa sÃ¼resi
```

#### Training Parameters
```python
--lr 5e-5                 # Learning rate
--gamma 0.99              # Discount factor
--lambda_ 0.95            # GAE lambda
--batch_size 4000         # PPO batch size
--mini_batch_size 512     # Mini-batch size
--epochs 5000             # Training epochs
```

#### High-Level Parameters
```python
--substeps_min 10         # Min substeps per high-level action
--substeps_max 20         # Max substeps per high-level action
--global_resolution 16    # Global observation resolution
```

---

## ğŸ“ Dosya YapÄ±sÄ±

```
hhmarl_2D-for-bee-colony/
â”œâ”€â”€ warsim/
â”‚   â”œâ”€â”€ simulator/
â”‚   â”‚   â”œâ”€â”€ bee.py                    # ArÄ± entity class'Ä±
â”‚   â”‚   â”œâ”€â”€ bee_simulator.py          # Ana simulator (build queue, rewards)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ grid_utils.py             # Grid utilities, flood fill
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ scenplotter/
â”‚       â”œâ”€â”€ bee_plotter.py            # Visualization
â”‚       â””â”€â”€ ...
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ env_bee_lowlevel.py           # Low-level environment
â”‚   â”œâ”€â”€ env_bee_highlevel.py          # High-level environment
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ac_models_bee.py              # Neural network models
â”‚   â””â”€â”€ ...
â”œâ”€â”€ config.py                          # BeeColonyConfig class
â”œâ”€â”€ train_bee_lowlevel.py             # Low-level training script
â”œâ”€â”€ train_bee_highlevel.py            # High-level training script
â”œâ”€â”€ test_bee_colony.py                # Test script
â”œâ”€â”€ requirements_bee_colony.txt       # Dependencies
â””â”€â”€ BEE_COLONY_README.md              # Bu dosya
```

---

## ğŸ¯ Reward YapÄ±sÄ± DetaylarÄ±

### 1. Enclosed Area Reward
```python
area_increase = current_area - previous_area
reward = area_increase * 0.5 / num_bees  # Her arÄ±ya daÄŸÄ±tÄ±lÄ±r
```

### 2. Adjacent Wall Bonus
```python
if new_wall_is_adjacent_to_existing_wall:
    reward += 0.1
```

### 3. Collaborative Building Bonus
```python
if multiple_bees_building_same_location:
    for each_bee:
        reward += 0.05
    build_time /= 2  # Her ek arÄ± sÃ¼reyi yarÄ±ya indirir
```

### 4. Penalties
```python
if building_on_completed_wall:
    reward -= 0.5
```

---

## ğŸ¨ Visualization

### Plotter KullanÄ±mÄ±

```python
from warsim.simulator.bee_simulator import BeeSimulator
from warsim.scenplotter.bee_plotter import BeePlotter

sim = BeeSimulator(num_bees=7, grid_size=500)
plotter = BeePlotter(grid_size=500, downsample=5)

# Her step'te
sim.do_tick(actions)
plotter.plot(sim, save_path="frame.png", show=True)
```

### GÃ¶rsel Ã–ÄŸeler
- ğŸ”µ **Mavi arÄ±lar**: Idle/hareket ediyor
- ğŸ”´ **KÄ±rmÄ±zÄ± arÄ±lar**: Ä°nÅŸa yapÄ±yor
- â¬› **Siyah kareler**: TamamlanmÄ±ÅŸ duvarlar
- ğŸŸ§ **Turuncu kareler**: Ä°nÅŸa devam ediyor
- ğŸŸ© **YeÅŸil bÃ¶lgeler**: KapalÄ± alanlar (enclosed areas)

---

## ğŸ“Š Training Tips

### 1. Curriculum Learning
Low-level training iÃ§in level'larÄ± sÄ±rayla eÄŸitin:
```bash
python train_bee_lowlevel.py --level 1 --epochs 1000
python train_bee_lowlevel.py --level 2 --epochs 1000
python train_bee_lowlevel.py --level 3 --epochs 1000
# ...
```

### 2. Hyperparameter Tuning
- Learning rate Ã§ok yÃ¼ksekse training unstable olur
- Batch size bÃ¼yÃ¼tmek genellikle daha stabil training saÄŸlar
- Horizon'u artÄ±rmak daha uzun vadeli stratejiler Ã¶ÄŸretir

### 3. Monitoring
TensorBoard ile training'i izleyin:
```bash
tensorboard --logdir results/
```

### 4. Checkpoint Management
Her 50 epoch'ta checkpoint kaydedilir. Ä°stediÄŸiniz checkpoint'ten devam edebilirsiniz:
```bash
python train_bee_lowlevel.py --restore --restore_path results/.../checkpoint
```

---

## ğŸ”¬ Algoritma DetaylarÄ±

### PPO (Proximal Policy Optimization)
- **Algorithm**: PPO-Clip
- **Policy Network**: Shared weights (all bees use same policy)
- **Critic Network**: Centralized (CTDE - Centralized Training, Decentralized Execution)
- **Advantage Estimation**: GAE (Generalized Advantage Estimation)

### CTDE (Centralized Training, Decentralized Execution)
- **Training**: Critic sees all bees' observations (centralized)
- **Execution**: Each bee only uses its own observation (decentralized)
- **Benefit**: Learns coordination while maintaining decentralized execution

---

## ğŸ› Troubleshooting

### Problem: OOM (Out of Memory)
**Ã‡Ã¶zÃ¼m**:
- `--batch_size` ve `--mini_batch_size` azaltÄ±n
- `--num_workers` azaltÄ±n
- Grid size'Ä± kÃ¼Ã§Ã¼ltÃ¼n

### Problem: Training Ã§ok yavaÅŸ
**Ã‡Ã¶zÃ¼m**:
- `--grid_size` kÃ¼Ã§Ã¼ltÃ¼n (500 â†’ 250)
- `--horizon` kÄ±saltÄ±n
- `--num_workers` artÄ±rÄ±n (eÄŸer CPU yeterliyse)

### Problem: Policy Ã¶ÄŸrenmiyor
**Ã‡Ã¶zÃ¼m**:
- Learning rate'i ayarlayÄ±n
- Reward scaling'i kontrol edin
- Daha uzun sÃ¼re eÄŸitin
- Simpler gÃ¶revlerle baÅŸlayÄ±n

---

## ğŸ“š Referanslar

Bu proje ÅŸu Ã§alÄ±ÅŸmalardan esinlenmiÅŸtir:
- QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL
- CommNet: Learning Multiagent Communication
- Feudal Networks for Hierarchical Reinforcement Learning

---

## ğŸ“ Citation

EÄŸer bu kodu kullanÄ±rsanÄ±z, lÃ¼tfen orijinal repository'yi cite edin:
```
@misc{bee_colony_marl,
  title={Bee Colony Honeycomb Construction with Hierarchical MARL},
  author={[Your Name]},
  year={2025},
  url={https://github.com/[username]/hhmarl_2D-for-bee-colony}
}
```

---

## ğŸ¤ Contributing

Pull request'ler memnuniyetle karÅŸÄ±lanÄ±r! Ã–nemli deÄŸiÅŸiklikler iÃ§in lÃ¼tfen Ã¶nce bir issue aÃ§Ä±n.

---

## ğŸ“§ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in issue aÃ§abilirsiniz.

---

**Ä°yi eÄŸitimler! ğŸğŸ¯**
