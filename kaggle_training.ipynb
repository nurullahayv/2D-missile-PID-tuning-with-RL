{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Missile PID Tuning with RL - Kaggle GPU Training\n",
    "\n",
    "Train reinforcement learning agents (PPO, SAC, TD3) to adaptively tune PID parameters for missile guidance.\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle GPU (T4 or P100)\n",
    "- Internet enabled (for pip install)\n",
    "\n",
    "**Training Time:**\n",
    "- PPO: ~30-45 minutes (1M timesteps)\n",
    "- SAC: ~45-60 minutes (1M timesteps)\n",
    "- TD3: ~45-60 minutes (1M timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n# Install dependencies with fixed versions\n# NumPy 2.x causes compatibility issues, pin to 1.x\n!pip install -q 'numpy<2' gymnasium 'stable-baselines3[extra]' torch tensorboard\n\nprint(\"\\nâœ“ Dependencies installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected! Training will be slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Missile PID System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\nclass PIDController:\n    \"\"\"Simple PID controller\"\"\"\n    def __init__(self, kp=2.0, ki=0.1, kd=0.5):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.integral = 0.0\n        self.prev_error = 0.0\n\n    def compute(self, error, dt):\n        self.integral += error * dt\n        derivative = (error - self.prev_error) / dt if dt > 0 else 0.0\n        output = self.kp * error + self.ki * self.integral + self.kd * derivative\n        self.prev_error = error\n        return output\n\n    def reset(self):\n        self.integral = 0.0\n        self.prev_error = 0.0\n\n    def set_gains(self, kp, ki, kd):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n\n\nclass Missile:\n    \"\"\"2D Missile with PID guidance - HIGH SPEED VERSION\"\"\"\n    def __init__(self, x=0.0, y=0.0, vx=800.0, vy=0.0,\n                 max_speed=1000.0, max_accel=1000.0,\n                 kp=2.0, ki=0.1, kd=0.5):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n        self.max_speed = max_speed\n        self.max_accel = max_accel\n        self.pid = PIDController(kp, ki, kd)\n        self.trajectory = [(x, y)]\n        self.active = True\n        self.fuel = 1.0\n\n    @property\n    def position(self):\n        return np.array([self.x, self.y])\n\n    @property\n    def velocity(self):\n        return np.array([self.vx, self.vy])\n\n    @property\n    def speed(self):\n        return np.linalg.norm(self.velocity)\n\n    @property\n    def heading(self):\n        return np.arctan2(self.vy, self.vx)\n\n    def update(self, target_pos, dt):\n        if not self.active:\n            return\n\n        dx = target_pos[0] - self.x\n        dy = target_pos[1] - self.y\n        desired_heading = np.arctan2(dy, dx)\n        error = desired_heading - self.heading\n        error = np.arctan2(np.sin(error), np.cos(error))\n\n        control = self.pid.compute(error, dt)\n        control = np.clip(control, -self.max_accel, self.max_accel)\n\n        if self.speed > 0:\n            perp = np.array([-self.vy, self.vx]) / self.speed\n            self.vx += perp[0] * control * dt\n            self.vy += perp[1] * control * dt\n\n        current_speed = self.speed\n        if current_speed > 0:\n            scale = self.max_speed / current_speed\n            self.vx *= scale\n            self.vy *= scale\n\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n        self.fuel -= 0.01 * dt\n        \n        if self.fuel <= 0:\n            self.active = False\n\n        self.trajectory.append((self.x, self.y))\n\n    def reset(self, x=0.0, y=0.0, vx=800.0, vy=0.0):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n        self.active = True\n        self.fuel = 1.0\n        self.trajectory = [(x, y)]\n        self.pid.reset()\n\n    def set_pid_gains(self, kp, ki, kd):\n        self.pid.set_gains(kp, ki, kd)\n\nprint(\"âœ“ Missile system defined (HIGH SPEED: 1000 m/s, 1000 m/sÂ²)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Moving Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Target:\n    \"\"\"Moving target with different maneuver patterns - HIGH SPEED VERSION\"\"\"\n    def __init__(self, x=8000.0, y=5000.0, speed=1000.0, maneuver='straight'):\n        self.x = x\n        self.y = y\n        self.speed = speed\n        self.heading = np.pi\n        self.maneuver = maneuver\n        self.time = 0.0\n        self.trajectory = [(x, y)]\n\n    @property\n    def position(self):\n        return np.array([self.x, self.y])\n\n    @property\n    def velocity(self):\n        vx = self.speed * np.cos(self.heading)\n        vy = self.speed * np.sin(self.heading)\n        return np.array([vx, vy])\n\n    def update(self, dt, missile_pos=None):\n        self.time += dt\n\n        if self.maneuver == 'straight':\n            pass\n        elif self.maneuver == 'circular':\n            turn_rate = 0.3\n            self.heading += turn_rate * dt\n        elif self.maneuver == 'zigzag':\n            if int(self.time) % 4 < 2:\n                turn_rate = 0.5\n            else:\n                turn_rate = -0.5\n            self.heading += turn_rate * dt\n        elif self.maneuver == 'evasive' and missile_pos is not None:\n            dx = self.x - missile_pos[0]\n            dy = self.y - missile_pos[1]\n            distance = np.sqrt(dx**2 + dy**2)\n            if distance < 3000:\n                escape_heading = np.arctan2(dy, dx)\n                heading_diff = escape_heading - self.heading\n                heading_diff = np.arctan2(np.sin(heading_diff), np.cos(heading_diff))\n                turn_rate = 0.8 * np.sign(heading_diff)\n                self.heading += turn_rate * dt\n\n        vx = self.speed * np.cos(self.heading)\n        vy = self.speed * np.sin(self.heading)\n        self.x += vx * dt\n        self.y += vy * dt\n        self.trajectory.append((self.x, self.y))\n\n    def reset(self, x=8000.0, y=5000.0, heading=np.pi):\n        self.x = x\n        self.y = y\n        self.heading = heading\n        self.time = 0.0\n        self.trajectory = [(x, y)]\n\nprint(\"âœ“ Target system defined (HIGH SPEED: 1000 m/s)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Gymnasium Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gymnasium as gym\nfrom gymnasium import spaces\n\nclass MissilePIDEnv(gym.Env):\n    \"\"\"Gym environment for missile PID tuning - HIGH SPEED VERSION\"\"\"\n    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 60}\n\n    def __init__(self, map_size=10000.0, hit_radius=50.0, max_steps=500,\n                 dt=0.01, target_maneuver='circular'):\n        super().__init__()\n        self.map_size = map_size\n        self.hit_radius = hit_radius\n        self.max_steps = max_steps\n        self.dt = dt\n        self.target_maneuver = target_maneuver\n\n        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(14,), dtype=np.float32)\n\n        self.kp_range = (0.1, 10.0)\n        self.ki_range = (0.0, 5.0)\n        self.kd_range = (0.0, 5.0)\n\n        self.missile = None\n        self.target = None\n        self.step_count = 0\n        self.prev_distance = None\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        missile_x = np.random.uniform(0, 2000)\n        missile_y = np.random.uniform(2000, 8000)\n        missile_vx = np.random.uniform(800, 900)  # Higher initial speed\n        missile_vy = 0.0\n\n        target_x = np.random.uniform(7000, 9000)\n        target_y = np.random.uniform(2000, 8000)\n        target_heading = np.random.uniform(0, 2*np.pi)\n\n        self.missile = Missile(x=missile_x, y=missile_y, vx=missile_vx, vy=missile_vy,\n                              max_speed=1000.0, max_accel=1000.0,\n                              kp=2.0, ki=0.1, kd=0.5)\n        self.target = Target(x=target_x, y=target_y, speed=1000.0, maneuver=self.target_maneuver)\n        self.target.heading = target_heading\n\n        self.step_count = 0\n        self.prev_distance = np.linalg.norm(self.missile.position - self.target.position)\n\n        return self._get_observation(), {}\n\n    def step(self, action):\n        self.step_count += 1\n\n        delta_kp, delta_ki, delta_kd = action\n        current_kp = self.missile.pid.kp\n        current_ki = self.missile.pid.ki\n        current_kd = self.missile.pid.kd\n\n        new_kp = np.clip(current_kp + delta_kp * 0.5, self.kp_range[0], self.kp_range[1])\n        new_ki = np.clip(current_ki + delta_ki * 0.2, self.ki_range[0], self.ki_range[1])\n        new_kd = np.clip(current_kd + delta_kd * 0.2, self.kd_range[0], self.kd_range[1])\n\n        self.missile.set_pid_gains(new_kp, new_ki, new_kd)\n        self.missile.update(self.target.position, self.dt)\n        self.target.update(self.dt, self.missile.position)\n\n        distance = np.linalg.norm(self.missile.position - self.target.position)\n        reward = -distance / self.map_size\n\n        if self.prev_distance is not None:\n            progress = (self.prev_distance - distance) / self.map_size\n            reward += progress * 10.0\n        self.prev_distance = distance\n\n        terminated = False\n        truncated = False\n\n        if distance < self.hit_radius:\n            reward += 100.0\n            terminated = True\n\n        if (not self.missile.active or\n            self.missile.x < 0 or self.missile.x > self.map_size or\n            self.missile.y < 0 or self.missile.y > self.map_size or\n            self.target.x < 0 or self.target.x > self.map_size or\n            self.target.y < 0 or self.target.y > self.map_size):\n            reward -= 50.0\n            terminated = True\n\n        if self.step_count >= self.max_steps:\n            truncated = True\n\n        info = {\n            'distance': distance,\n            'hit': distance < self.hit_radius,\n            'step': self.step_count,\n            'pid_gains': {'kp': self.missile.pid.kp, 'ki': self.missile.pid.ki, 'kd': self.missile.pid.kd}\n        }\n\n        return self._get_observation(), reward, terminated, truncated, info\n\n    def _get_observation(self):\n        missile_pos = self.missile.position\n        missile_vel = self.missile.velocity\n        target_pos = self.target.position\n        target_vel = self.target.velocity\n\n        dx = target_pos[0] - missile_pos[0]\n        dy = target_pos[1] - missile_pos[1]\n        distance = np.sqrt(dx**2 + dy**2)\n\n        desired_heading = np.arctan2(dy, dx)\n        current_heading = self.missile.heading\n        angle_error = desired_heading - current_heading\n        angle_error = np.arctan2(np.sin(angle_error), np.cos(angle_error))\n\n        obs = np.array([\n            missile_pos[0] / self.map_size,\n            missile_pos[1] / self.map_size,\n            missile_vel[0] / self.missile.max_speed,\n            missile_vel[1] / self.missile.max_speed,\n            target_pos[0] / self.map_size,\n            target_pos[1] / self.map_size,\n            target_vel[0] / self.target.speed,\n            target_vel[1] / self.target.speed,\n            distance / self.map_size,\n            angle_error / np.pi,\n            self.missile.pid.kp / 10.0,\n            self.missile.pid.ki / 5.0,\n            self.missile.pid.kd / 5.0,\n            self.missile.fuel\n        ], dtype=np.float32)\n\n        return obs\n\nprint(\"âœ“ Gymnasium environment defined (HIGH SPEED: 1000 m/s both)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration - HIGH SPEED + DEEP SAC\nALGORITHM = 'SAC'  # Deep Soft Actor-Critic\nTARGET_MANEUVER = 'circular'  # Circular maneuver\nTOTAL_TIMESTEPS = 1_000_000  # 1M timesteps\nN_ENVS = 1  # Single environment\n\nprint(f\"Configuration:\")\nprint(f\"  Algorithm: {ALGORITHM} (Deep RL)\")\nprint(f\"  Target Maneuver: {TARGET_MANEUVER}\")\nprint(f\"  Total Timesteps: {TOTAL_TIMESTEPS:,}\")\nprint(f\"  Environment: Single (DummyVecEnv)\")\nprint(f\"\\nðŸš€ HIGH SPEED MODE:\")\nprint(f\"  Missile: 1000 m/s, 1000 m/sÂ²\")\nprint(f\"  Target: 1000 m/s\")\nprint(f\"\\nðŸ§  DEEP RL: 3-layer networks (256 units each)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from stable_baselines3 import PPO, SAC, TD3\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\nimport os\nfrom datetime import datetime\n\ndef make_env(rank, maneuver):\n    \"\"\"Create environment instance\"\"\"\n    def _init():\n        return MissilePIDEnv(\n            map_size=10000.0,\n            hit_radius=50.0,\n            max_steps=500,\n            dt=0.01,\n            target_maneuver=maneuver\n        )\n    return _init\n\n# Create vectorized environments (use DummyVecEnv to avoid multiprocessing issues)\nenv = DummyVecEnv([make_env(0, TARGET_MANEUVER)])\n\n# Create eval environment\neval_env = DummyVecEnv([make_env(0, TARGET_MANEUVER)])\n\nprint(f\"âœ“ Training environment created\")\nprint(f\"âœ“ Using DummyVecEnv (single process, no multiprocessing issues)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create output directory\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nrun_name = f\"{ALGORITHM}_{TARGET_MANEUVER}_{timestamp}\"\nrun_dir = f\"/kaggle/working/{run_name}\"\nos.makedirs(run_dir, exist_ok=True)\n\n# Deep RL network architecture\n# 3 hidden layers, 256 units each (deeper than default 2x256)\npolicy_kwargs = dict(\n    net_arch=dict(\n        pi=[256, 256, 256],  # Actor network: 3 layers\n        qf=[256, 256, 256]   # Critic network: 3 layers\n    )\n)\n\n# Initialize algorithm\nif ALGORITHM == 'PPO':\n    model = PPO(\n        'MlpPolicy',\n        env,\n        learning_rate=3e-4,\n        n_steps=2048,\n        batch_size=64,\n        n_epochs=10,\n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_range=0.2,\n        ent_coef=0.01,\n        verbose=1,\n        tensorboard_log=os.path.join(run_dir, 'tensorboard'),\n        device='cuda',\n        policy_kwargs=policy_kwargs\n    )\nelif ALGORITHM == 'SAC':\n    model = SAC(\n        'MlpPolicy',\n        env,\n        learning_rate=3e-4,\n        buffer_size=100_000,\n        learning_starts=1000,\n        batch_size=256,\n        tau=0.005,\n        gamma=0.99,\n        train_freq=1,\n        gradient_steps=1,\n        ent_coef='auto',\n        verbose=1,\n        tensorboard_log=os.path.join(run_dir, 'tensorboard'),\n        device='cuda',\n        policy_kwargs=policy_kwargs  # Deep network\n    )\nelif ALGORITHM == 'TD3':\n    model = TD3(\n        'MlpPolicy',\n        env,\n        learning_rate=3e-4,\n        buffer_size=100_000,\n        learning_starts=1000,\n        batch_size=256,\n        tau=0.005,\n        gamma=0.99,\n        train_freq=1,\n        gradient_steps=1,\n        policy_delay=2,\n        verbose=1,\n        tensorboard_log=os.path.join(run_dir, 'tensorboard'),\n        device='cuda',\n        policy_kwargs=policy_kwargs\n    )\nelse:\n    raise ValueError(f\"Unknown algorithm: {ALGORITHM}\")\n\nprint(f\"âœ“ {ALGORITHM} model initialized on GPU\")\nprint(f\"âœ“ Deep RL: 3-layer networks (256â†’256â†’256)\")\nprint(f\"âœ“ Output directory: {run_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback - save model periodically\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=50_000 // N_ENVS,\n",
    "    save_path=os.path.join(run_dir, 'checkpoints'),\n",
    "    name_prefix='model'\n",
    ")\n",
    "\n",
    "# Evaluation callback - evaluate and save best model\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=os.path.join(run_dir, 'best_model'),\n",
    "    log_path=os.path.join(run_dir, 'eval_logs'),\n",
    "    eval_freq=10_000 // N_ENVS,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model ðŸš€\n",
    "\n",
    "This will take 30-60 minutes depending on GPU and algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training {ALGORITHM} on {TARGET_MANEUVER} target\")\n",
    "print(f\"Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Train!\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=[checkpoint_callback, eval_callback],\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_path = os.path.join(run_dir, 'final_model')\n",
    "model.save(final_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ Training complete!\")\n",
    "print(f\"âœ“ Final model saved: {final_path}\")\n",
    "print(f\"âœ“ Best model saved: {os.path.join(run_dir, 'best_model')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join(run_dir, 'best_model', 'best_model.zip')\n",
    "\n",
    "if ALGORITHM == 'PPO':\n",
    "    eval_model = PPO.load(best_model_path)\n",
    "elif ALGORITHM == 'SAC':\n",
    "    eval_model = SAC.load(best_model_path)\n",
    "elif ALGORITHM == 'TD3':\n",
    "    eval_model = TD3.load(best_model_path)\n",
    "\n",
    "# Evaluate\n",
    "n_eval_episodes = 20\n",
    "eval_env_single = MissilePIDEnv(\n",
    "    map_size=10000.0,\n",
    "    hit_radius=50.0,\n",
    "    max_steps=500,\n",
    "    dt=0.01,\n",
    "    target_maneuver=TARGET_MANEUVER\n",
    ")\n",
    "\n",
    "hits = 0\n",
    "total_rewards = []\n",
    "distances = []\n",
    "steps_list = []\n",
    "\n",
    "print(f\"\\nEvaluating model on {n_eval_episodes} episodes...\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    obs, info = eval_env_single.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = eval_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env_single.step(action)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "    distances.append(info['distance'])\n",
    "    steps_list.append(step)\n",
    "\n",
    "    if info['hit']:\n",
    "        hits += 1\n",
    "        status = \"âœ“ HIT\"\n",
    "    else:\n",
    "        status = \"âœ— MISS\"\n",
    "\n",
    "    print(f\"Episode {episode+1:2d}: {status} | \"\n",
    "          f\"Distance: {info['distance']:6.1f}m | \"\n",
    "          f\"Reward: {episode_reward:7.1f} | \"\n",
    "          f\"Steps: {step:3d} | \"\n",
    "          f\"PID: Kp={info['pid_gains']['kp']:.2f} \"\n",
    "          f\"Ki={info['pid_gains']['ki']:.2f} \"\n",
    "          f\"Kd={info['pid_gains']['kd']:.2f}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nðŸ“Š Evaluation Summary:\")\n",
    "print(f\"  Hit rate: {hits}/{n_eval_episodes} ({100*hits/n_eval_episodes:.1f}%)\")\n",
    "print(f\"  Avg reward: {np.mean(total_rewards):.1f} Â± {np.std(total_rewards):.1f}\")\n",
    "print(f\"  Avg distance: {np.mean(distances):.1f}m Â± {np.std(distances):.1f}m\")\n",
    "print(f\"  Avg steps: {np.mean(steps_list):.1f} Â± {np.std(steps_list):.1f}\")\n",
    "print(f\"  Min distance: {np.min(distances):.1f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load tensorboard logs\n",
    "eval_log_path = os.path.join(run_dir, 'eval_logs', 'evaluations.npz')\n",
    "\n",
    "if os.path.exists(eval_log_path):\n",
    "    eval_data = np.load(eval_log_path)\n",
    "    timesteps = eval_data['timesteps']\n",
    "    results = eval_data['results']\n",
    "    \n",
    "    mean_rewards = results.mean(axis=1)\n",
    "    std_rewards = results.std(axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(timesteps, mean_rewards, label='Mean Reward', linewidth=2)\n",
    "    ax.fill_between(timesteps, \n",
    "                     mean_rewards - std_rewards, \n",
    "                     mean_rewards + std_rewards, \n",
    "                     alpha=0.3, label='Â± Std Dev')\n",
    "    ax.set_xlabel('Timesteps', fontsize=12)\n",
    "    ax.set_ylabel('Episode Reward', fontsize=12)\n",
    "    ax.set_title(f'{ALGORITHM} Training Progress on {TARGET_MANEUVER.capitalize()} Target', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(run_dir, 'training_progress.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Training plot saved to {os.path.join(run_dir, 'training_progress.png')}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Evaluation logs not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare with Fixed PID Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fixed PID baseline\n",
    "baseline_env = MissilePIDEnv(\n",
    "    map_size=10000.0,\n",
    "    hit_radius=50.0,\n",
    "    max_steps=500,\n",
    "    dt=0.01,\n",
    "    target_maneuver=TARGET_MANEUVER\n",
    ")\n",
    "\n",
    "baseline_hits = 0\n",
    "baseline_distances = []\n",
    "\n",
    "print(\"\\nTesting Fixed PID Baseline (Kp=2.0, Ki=0.1, Kd=0.5)...\\n\")\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    obs, info = baseline_env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # No RL - just keep PID gains fixed\n",
    "    baseline_env.missile.set_pid_gains(2.0, 0.1, 0.5)\n",
    "    \n",
    "    while not done:\n",
    "        # Zero action = no PID adjustment\n",
    "        obs, reward, terminated, truncated, info = baseline_env.step(np.zeros(3))\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    if info['hit']:\n",
    "        baseline_hits += 1\n",
    "    baseline_distances.append(info['distance'])\n",
    "\n",
    "print(f\"\\nðŸ“Š Comparison:\")\n",
    "print(f\"  Fixed PID Hit Rate: {baseline_hits}/{n_eval_episodes} ({100*baseline_hits/n_eval_episodes:.1f}%)\")\n",
    "print(f\"  RL Agent Hit Rate:  {hits}/{n_eval_episodes} ({100*hits/n_eval_episodes:.1f}%)\")\n",
    "print(f\"  Improvement: {100*(hits - baseline_hits)/n_eval_episodes:+.1f}%\")\n",
    "print(f\"\\n  Fixed PID Avg Distance: {np.mean(baseline_distances):.1f}m\")\n",
    "print(f\"  RL Agent Avg Distance:  {np.mean(distances):.1f}m\")\n",
    "print(f\"  Improvement: {np.mean(baseline_distances) - np.mean(distances):.1f}m closer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file of trained model\n",
    "import shutil\n",
    "\n",
    "archive_name = f\"{run_name}_trained_model\"\n",
    "shutil.make_archive(\n",
    "    f\"/kaggle/working/{archive_name}\",\n",
    "    'zip',\n",
    "    run_dir\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model archived: /kaggle/working/{archive_name}.zip\")\n",
    "print(f\"\\nðŸ“¥ Download from Kaggle Output panel\")\n",
    "print(f\"\\nContents:\")\n",
    "print(f\"  - best_model/      (Best performing model)\")\n",
    "print(f\"  - final_model.zip  (Final model after training)\")\n",
    "print(f\"  - checkpoints/     (Periodic checkpoints)\")\n",
    "print(f\"  - eval_logs/       (Evaluation metrics)\")\n",
    "print(f\"  - tensorboard/     (Training logs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### What we did:\n",
    "1. âœ… Set up GPU environment\n",
    "2. âœ… Defined missile PID system\n",
    "3. âœ… Created moving target with maneuvers\n",
    "4. âœ… Built Gymnasium RL environment\n",
    "5. âœ… Trained RL agent (PPO/SAC/TD3) on GPU\n",
    "6. âœ… Evaluated performance\n",
    "7. âœ… Compared with fixed PID baseline\n",
    "8. âœ… Visualized training progress\n",
    "\n",
    "### Next steps:\n",
    "- Download trained model\n",
    "- Test on different maneuvers\n",
    "- Try other algorithms (change ALGORITHM variable)\n",
    "- Adjust hyperparameters\n",
    "- Train longer (increase TOTAL_TIMESTEPS)\n",
    "\n",
    "### To use trained model locally:\n",
    "```python\n",
    "from stable_baselines3 import PPO  # or SAC, TD3\n",
    "model = PPO.load('path/to/best_model.zip')\n",
    "obs, info = env.reset()\n",
    "action, _ = model.predict(obs, deterministic=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}