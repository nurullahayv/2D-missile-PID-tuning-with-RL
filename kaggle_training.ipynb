{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missile PID Tuning with Reinforcement Learning - GPU Training\n",
    "\n",
    "Bu notebook, 2D savaş ortamında manevra yapan hedeflere karşı füze otopilotu PID parametrelerinin pekiştirmeli öğrenme ile adaptif ayarlanması için Kaggle GPU ortamında eğitim yapmak üzere tasarlanmıştır.\n",
    "\n",
    "## Kurulum\n",
    "\n",
    "1. Bu notebook'u Kaggle'a yükleyin\n",
    "2. Settings > Accelerator > GPU'yu etkinleştirin\n",
    "3. Internet'i etkinleştirin (Add Data > Internet)\n",
    "4. Çalıştırın!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repoyu Klonlama ve Gerekli Paketleri Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repoyu klonla\n",
    "!git clone https://github.com/YOUR_USERNAME/2D-missile-PID-tuning-with-RL.git\n",
    "%cd 2D-missile-PID-tuning-with-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli paketleri yükle\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU kontrolü\n",
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfigürasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Import environment\n",
    "from envs.missile_pid_env import MissilePIDEnv\n",
    "from config import get_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim parametreleri\n",
    "CONFIG = {\n",
    "    # Environment\n",
    "    'max_steps': 500,\n",
    "    'dt': 0.1,\n",
    "    'map_size': 10000.0,\n",
    "    'hit_radius': 50.0,\n",
    "    'target_maneuver': 'circular',  # 'straight', 'circular', 'zigzag', 'evasive'\n",
    "    \n",
    "    # Training\n",
    "    'algorithm': 'PPO',  # 'PPO', 'SAC', 'TD3'\n",
    "    'total_timesteps': 2_000_000,  # Kaggle'da GPU ile daha hızlı\n",
    "    'learning_rate': 3e-4,\n",
    "    'batch_size': 128,\n",
    "    'n_steps': 2048,\n",
    "    'gamma': 0.99,\n",
    "    \n",
    "    # Model\n",
    "    'hidden_size': 256,\n",
    "    'n_layers': 2,\n",
    "    \n",
    "    # Other\n",
    "    'seed': 42,\n",
    "    'device': 'cuda',\n",
    "    'n_envs': 8,  # Paralel environment sayısı\n",
    "    'save_freq': 50000,\n",
    "    'eval_freq': 10000,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment\n",
    "env = MissilePIDEnv(\n",
    "    max_steps=CONFIG['max_steps'],\n",
    "    dt=CONFIG['dt'],\n",
    "    map_size=CONFIG['map_size'],\n",
    "    hit_radius=CONFIG['hit_radius'],\n",
    "    target_maneuver=CONFIG['target_maneuver']\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Test random actions\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step: reward={reward:.2f}, terminated={terminated}, truncated={truncated}\")\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"\\nEnvironment test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dizinleri oluştur\n",
    "log_dir = f\"./logs/{CONFIG['algorithm']}_{CONFIG['target_maneuver']}\"\n",
    "save_dir = f\"./models/{CONFIG['algorithm']}_{CONFIG['target_maneuver']}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Save directory: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized training environment\n",
    "env_kwargs = {\n",
    "    'max_steps': CONFIG['max_steps'],\n",
    "    'dt': CONFIG['dt'],\n",
    "    'map_size': CONFIG['map_size'],\n",
    "    'hit_radius': CONFIG['hit_radius'],\n",
    "    'target_maneuver': CONFIG['target_maneuver'],\n",
    "}\n",
    "\n",
    "train_env = make_vec_env(\n",
    "    lambda: MissilePIDEnv(**env_kwargs),\n",
    "    n_envs=CONFIG['n_envs'],\n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "# Evaluation environment\n",
    "eval_env = MissilePIDEnv(**env_kwargs)\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "print(f\"Created {CONFIG['n_envs']} parallel training environments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model oluştur\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[CONFIG['hidden_size']] * CONFIG['n_layers']\n",
    ")\n",
    "\n",
    "if CONFIG['algorithm'] == \"PPO\":\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        n_steps=CONFIG['n_steps'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        gamma=CONFIG['gamma'],\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=CONFIG['device'],\n",
    "        seed=CONFIG['seed']\n",
    "    )\n",
    "elif CONFIG['algorithm'] == \"SAC\":\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        gamma=CONFIG['gamma'],\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=CONFIG['device'],\n",
    "        seed=CONFIG['seed']\n",
    "    )\n",
    "elif CONFIG['algorithm'] == \"TD3\":\n",
    "    model = TD3(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        gamma=CONFIG['gamma'],\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=CONFIG['device'],\n",
    "        seed=CONFIG['seed']\n",
    "    )\n",
    "\n",
    "print(f\"Created {CONFIG['algorithm']} model on {CONFIG['device']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.policy.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=CONFIG['save_freq'],\n",
    "    save_path=save_dir,\n",
    "    name_prefix=\"model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=save_dir,\n",
    "    log_path=log_dir,\n",
    "    eval_freq=CONFIG['eval_freq'],\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "print(\"Callbacks configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EĞİTİM BAŞLAT!\n",
    "print(\"=\"*60)\n",
    "print(f\"Starting training: {CONFIG['total_timesteps']:,} timesteps\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=CONFIG['total_timesteps'],\n",
    "    callback=[checkpoint_callback, eval_callback],\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final modeli kaydet\n",
    "final_model_path = os.path.join(save_dir, \"final_model\")\n",
    "model.save(final_model_path)\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment\n",
    "test_env = MissilePIDEnv(**env_kwargs)\n",
    "\n",
    "n_test_episodes = 10\n",
    "episode_rewards = []\n",
    "hit_success = []\n",
    "\n",
    "print(f\"\\nTesting model on {n_test_episodes} episodes...\")\n",
    "\n",
    "for episode in range(n_test_episodes):\n",
    "    obs, info = test_env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    hit_success.append(info.get('hit', False))\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Reward={episode_reward:.2f}, Hit={'Yes' if info.get('hit', False) else 'No'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Average Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"Hit Success Rate: {np.mean(hit_success)*100:.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one episode\n",
    "obs, info = test_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "# Plot trajectory\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Missile trajectory\n",
    "missile_traj = np.array(test_env.missile.trajectory)\n",
    "ax.plot(missile_traj[:, 0], missile_traj[:, 1], 'b-', linewidth=2, label='Missile', alpha=0.7)\n",
    "ax.plot(missile_traj[0, 0], missile_traj[0, 1], 'bo', markersize=12, label='Missile Start')\n",
    "ax.plot(missile_traj[-1, 0], missile_traj[-1, 1], 'bs', markersize=12, label='Missile End')\n",
    "\n",
    "# Target trajectory\n",
    "target_traj = np.array(test_env.target.trajectory)\n",
    "ax.plot(target_traj[:, 0], target_traj[:, 1], 'r-', linewidth=2, label='Target', alpha=0.7)\n",
    "ax.plot(target_traj[0, 0], target_traj[0, 1], 'ro', markersize=12, label='Target Start')\n",
    "ax.plot(target_traj[-1, 0], target_traj[-1, 1], 'rs', markersize=12, label='Target End')\n",
    "\n",
    "# Hit radius\n",
    "circle = plt.Circle((target_traj[-1, 0], target_traj[-1, 1]),\n",
    "                    test_env.hit_radius, color='r', fill=False,\n",
    "                    linestyle='--', linewidth=2, label='Hit Radius')\n",
    "ax.add_patch(circle)\n",
    "\n",
    "ax.set_xlabel('X Position (m)', fontsize=12)\n",
    "ax.set_ylabel('Y Position (m)', fontsize=12)\n",
    "ax.set_title(f'Missile vs {CONFIG[\"target_maneuver\"].capitalize()} Target', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trajectory.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Hit: {'Yes' if info.get('hit', False) else 'No'}\")\n",
    "print(f\"Final distance: {info.get('distance', 0):.2f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model'i İndirme\n",
    "\n",
    "Eğitilmiş modeli ve logları indirmek için Kaggle Output bölümünden alabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dosyalarını ziple\n",
    "!zip -r trained_model.zip {save_dir}\n",
    "!zip -r logs.zip {log_dir}\n",
    "\n",
    "print(\"Model ve loglar ziplenmiştir. Output sekmesinden indirebilirsiniz.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
