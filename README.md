# 2D Missile PID Tuning with Reinforcement Learning

**Temiz, minimal implementation** - 2D fÃ¼ze gÃ¼dÃ¼m sistemi, RL ile adaptif PID parametre ayarlama.

## ğŸ¯ AmaÃ§

- **GÃ¶rev**: 2D fÃ¼ze (PID kontrollÃ¼) â†’ hareketli hedefi takip et
- **RL Hedefi**: PID parametrelerini (Kp, Ki, Kd) adaptif olarak ayarla
- **Test**: FarklÄ± RL algoritmalarÄ±nÄ± (PPO, SAC, TD3) karÅŸÄ±laÅŸtÄ±r

## ğŸ“¦ Stack

- **Gymnasium**: RL environment
- **Pygame**: GÃ¶rselleÅŸtirme
- **PyTorch**: Neural network backend
- **Stable-Baselines3**: RL algorithms (PPO, SAC, TD3)

## ğŸ—ï¸ YapÄ±

```
src/
  missile.py           # PID kontrollÃ¼ fÃ¼ze
  target.py            # Hareketli hedef (4 manevra tipi)
  environment.py       # Gym environment (adaptif PID)
  fixed_pid_env.py     # Fixed PID environment (non-adaptif)
  renderer.py          # Pygame gÃ¶rselleÅŸtirme
train.py               # RL training (adaptif PID)
train_fixed_pid.py     # RL training (fixed PID) â­
evaluate.py            # Model evaluation
demo.py                # Basit demo (RL yok)
kaggle_training.ipynb  # Kaggle GPU training notebook ğŸ®
config.yaml            # KonfigÃ¼rasyon
```

## ğŸš€ Kurulum

```bash
pip install -r requirements.txt
```

## ğŸ’» KullanÄ±m

### 0. ğŸ® Kaggle GPU Training (Ã–nerilen!)

**En hÄ±zlÄ± yol: Kaggle'da Ã¼cretsiz GPU ile eÄŸit!**

1. Kaggle'a git: https://www.kaggle.com
2. `kaggle_training.ipynb` dosyasÄ±nÄ± upload et
3. Settings â†’ Accelerator â†’ **GPU T4** seÃ§
4. "Run All" - 30-45 dakikada model hazÄ±r!
5. Trained modeli indir

**Avantajlar:**
- âœ… Ãœcretsiz GPU (T4/P100)
- âœ… Kurulum yok, direkt Ã§alÄ±ÅŸÄ±r
- âœ… 30-45 dakikada eÄŸitim tamamlanÄ±r
- âœ… Trained model indirilebilir

### 1. Demo (RL olmadan, sabit PID)

```bash
# Dairesel manevra yapan hedef
python demo.py --maneuver circular --kp 2.0 --ki 0.1 --kd 0.5

# KaÃ§an hedef
python demo.py --maneuver evasive --kp 3.0 --ki 0.15 --kd 0.8

# DÃ¼z giden hedef (kolay)
python demo.py --maneuver straight --kp 1.5 --ki 0.05 --kd 0.3

# Zigzag yapan hedef
python demo.py --maneuver zigzag --kp 2.5 --ki 0.12 --kd 0.6
```

### 2. RL Training

#### A) Fixed PID Training â­ (Ã–nerilen)

**En pratik yaklaÅŸÄ±m**: RL ile optimal **sabit** PID parametrelerini bul

```bash
# SAC ile circular hedef iÃ§in optimal PID bul
python train_fixed_pid.py --algorithm SAC --maneuver circular --timesteps 500000

# Evasive hedef iÃ§in
python train_fixed_pid.py --algorithm SAC --maneuver evasive --timesteps 500000

# FarklÄ± hÄ±zlarda test et
python train_fixed_pid.py --algorithm SAC --maneuver circular \
    --missile_speed 1000 --missile_accel 1000 --target_speed 1000 \
    --timesteps 500000
```

**Ã‡Ä±ktÄ±**: Script otomatik olarak optimal PID parametrelerini bulur ve terminale yazdÄ±rÄ±r:
```
Optimal PID Parameters for 'circular' target:
  Kp = 3.245 Â± 0.123
  Ki = 0.187 Â± 0.042
  Kd = 0.712 Â± 0.089

ğŸ’¡ Use these values in demo.py:
   python demo.py --maneuver circular --kp 3.245 --ki 0.187 --kd 0.712
```

**Avantajlar**:
- âœ… GerÃ§ek fÃ¼ze sistemlerine benzer (sabit PID)
- âœ… Yorumlanabilir sonuÃ§lar (somut PID deÄŸerleri)
- âœ… Daha hÄ±zlÄ± Ã¶ÄŸrenme
- âœ… Demo'da test edilebilir

#### B) Adaptive PID Training (Ä°leri seviye)

**Dinamik sistem**: RL her adÄ±mda PID parametrelerini deÄŸiÅŸtirir

```bash
# PPO ile eÄŸit (dairesel hedef)
python train.py --algorithm PPO --maneuver circular --timesteps 1000000

# SAC ile eÄŸit (kaÃ§an hedef)
python train.py --algorithm SAC --maneuver evasive --timesteps 1000000 --n_envs 8

# TD3 ile eÄŸit (zigzag hedef)
python train.py --algorithm TD3 --maneuver zigzag --timesteps 500000
```

**Output**: `models/` klasÃ¶rÃ¼ne kaydedilir

### 3. Trained Model Evaluation

```bash
# GÃ¶rselleÅŸtirme ile
python evaluate.py models/PPO_circular_*/best_model/best_model.zip --render --n_episodes 10

# Sadece metrikler
python evaluate.py models/SAC_evasive_*/final_model.zip --n_episodes 20
```

## ğŸ“Š Sistem DetaylarÄ±

### FÃ¼ze
- **State**: Pozisyon (x, y), HÄ±z (vx, vy)
- **KontrolÃ¶r**: PID (heading kontrolÃ¼)
- **KÄ±sÄ±tlar**:
  - Default: max_speed=300m/s, max_accel=100m/sÂ²
  - High-speed: max_speed=1000m/s, max_accel=1000m/sÂ²
- **Fizik**: 100 Hz gÃ¼ncelleme (dt=0.01s)

### Hedef
- **HÄ±z**:
  - Default: 150 m/s
  - High-speed: 1000 m/s
- **Manevralar**:
  - `straight`: Manevra yok
  - `circular`: Sabit dÃ¶nÃ¼ÅŸ hÄ±zÄ±
  - `zigzag`: Periyodik yÃ¶n deÄŸiÅŸimleri
  - `evasive`: FÃ¼zeye tepkisel kaÃ§Ä±ÅŸ

### RL Environment

#### Adaptive PID Environment (`environment.py`)
**Observation (14D)**:
- FÃ¼ze: pozisyon, hÄ±z, PID gains, fuel
- Hedef: pozisyon, hÄ±z
- Relative: mesafe, aÃ§Ä± hatasÄ±

**Action (3D continuous)**:
- `[Î”kp, Î”ki, Î”kd]` âˆˆ [-1, 1]Â³ (her adÄ±mda deÄŸiÅŸiklik)

**Reward**:
- -distance (normalize edilmiÅŸ)
- +hedefe yaklaÅŸma bonusu
- +100 (vurdu)
- -50 (Ä±skaladÄ±)

#### Fixed PID Environment (`fixed_pid_env.py`) â­
**Observation (11D)**:
- FÃ¼ze: pozisyon, hÄ±z, fuel
- Hedef: pozisyon, hÄ±z
- Relative: mesafe, aÃ§Ä± hatasÄ±
- (PID gains gÃ¶zlenmez Ã§Ã¼nkÃ¼ sabit)

**Action (3D continuous)**:
- `[Kp, Ki, Kd]` - Direkt PID deÄŸerleri
- Sadece episode baÅŸÄ±nda bir kere seÃ§ilir, sonra sabit kalÄ±r

**Reward**:
- Episode sonunda toplam reward
- Hit rate ve intercept sÃ¼resi odaklÄ±

### Desteklenen Algoritmalar
- **PPO**: On-policy, stabil, iyi baseline
- **SAC**: Off-policy, sample-efficient
- **TD3**: Off-policy, deterministic, robust

## ğŸ“ˆ Beklenen SonuÃ§lar

| Method    | Maneuver  | Hit Rate | Avg Steps |
|-----------|-----------|----------|-----------|
| Sabit PID | Straight  | ~90%     | 120       |
| Sabit PID | Circular  | ~70%     | 180       |
| Sabit PID | Evasive   | ~40%     | 250       |
| RL (PPO)  | Circular  | ~85%     | 150       |
| RL (SAC)  | Evasive   | ~65%     | 200       |

RL ajanlarÄ± zor manevralarda **+10-20% iyileÅŸtirme** gÃ¶stermeli.

## âš™ï¸ KonfigÃ¼rasyon

`config.yaml` dosyasÄ±nÄ± dÃ¼zenle:
- Harita boyutu, vuruÅŸ yarÄ±Ã§apÄ±
- FÃ¼ze/hedef hÄ±zlarÄ±
- PID default deÄŸerleri ve aralÄ±klarÄ±
- Training hyperparameters

## ğŸ¨ GÃ¶rselleÅŸtirme

Pygame renderer gÃ¶sterir:
- FÃ¼ze (cyan) ve hedef (red)
- Trajectory'ler (son 100 nokta)
- VuruÅŸ yarÄ±Ã§apÄ± Ã§emberi
- Real-time info: mesafe, PID gains, fuel, hÄ±z

Kontroller:
- **ESC** veya **Q**: Ã‡Ä±kÄ±ÅŸ

## ğŸ”§ Ä°leri KullanÄ±m

### Paralel Training

```bash
# Daha fazla paralel environment
python train.py --algorithm PPO --n_envs 8 --timesteps 2000000
```

### Hyperparameter Tuning

`train.py` iÃ§inde deÄŸiÅŸtir:
- Learning rate
- Batch size
- Network architecture

### Custom Maneuvers

`src/target.py` iÃ§inde yeni manevra ekle:

```python
elif self.maneuver == 'custom':
    # Kendi manevralarÄ±nÄ±z
    pass
```

## ğŸ› Sorun Giderme

**YavaÅŸ rendering**: `demo.py` veya `evaluate.py` Ã§alÄ±ÅŸtÄ±rÄ±rken `--render` kullanma

**Training converge olmuyor**:
- Timesteps artÄ±r
- `src/environment.py` iÃ§inde reward function ayarla
- FarklÄ± algoritma dene (SAC genelde daha sample-efficient)

**Import errors**: Proje root'undan Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan ve `requirements.txt` yÃ¼klendiÄŸinden emin olun

## ğŸ“š Kaynaklar

1. **Control Systems**: Franklin et al., "Feedback Control of Dynamic Systems"
2. **RL**: Sutton & Barto, "Reinforcement Learning: An Introduction"
3. **PPO**: Schulman et al., "Proximal Policy Optimization Algorithms"
4. **SAC**: Haarnoja et al., "Soft Actor-Critic"
5. **TD3**: Fujimoto et al., "Addressing Function Approximation Error"

---

**Akademik kontrol sistemleri dÃ¶kÃ¼mantasyonu iÃ§in**: `CONTROL_SYSTEM_ARCHITECTURE.md`
