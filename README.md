# 2D Missile PID Tuning with Reinforcement Learning

**Temiz, minimal implementation** - 2D fÃ¼ze gÃ¼dÃ¼m sistemi, RL ile adaptif PID parametre ayarlama.

## ğŸ¯ AmaÃ§

- **GÃ¶rev**: 2D fÃ¼ze (PID kontrollÃ¼) â†’ hareketli hedefi takip et
- **RL Hedefi**: PID parametrelerini (Kp, Ki, Kd) adaptif olarak ayarla
- **Test**: FarklÄ± RL algoritmalarÄ±nÄ± (PPO, SAC, TD3) karÅŸÄ±laÅŸtÄ±r

## ğŸ“¦ Stack

- **Gymnasium**: RL environment
- **Pygame**: GÃ¶rselleÅŸtirme
- **PyTorch**: Neural network backend
- **Stable-Baselines3**: RL algorithms (PPO, SAC, TD3)

## ğŸ—ï¸ YapÄ±

```
src/
  missile.py      # PID kontrollÃ¼ fÃ¼ze
  target.py       # Hareketli hedef (4 manevra tipi)
  environment.py  # Gym environment
  renderer.py     # Pygame gÃ¶rselleÅŸtirme
train.py          # RL training
evaluate.py       # Model evaluation
demo.py           # Basit demo (RL yok)
config.yaml       # KonfigÃ¼rasyon
```

## ğŸš€ Kurulum

```bash
pip install -r requirements.txt
```

## ğŸ’» KullanÄ±m

### 1. Demo (RL olmadan, sabit PID)

```bash
# Dairesel manevra yapan hedef
python demo.py --maneuver circular --kp 2.0 --ki 0.1 --kd 0.5

# KaÃ§an hedef
python demo.py --maneuver evasive --kp 3.0 --ki 0.15 --kd 0.8

# DÃ¼z giden hedef (kolay)
python demo.py --maneuver straight --kp 1.5 --ki 0.05 --kd 0.3

# Zigzag yapan hedef
python demo.py --maneuver zigzag --kp 2.5 --ki 0.12 --kd 0.6
```

### 2. RL Training

```bash
# PPO ile eÄŸit (dairesel hedef)
python train.py --algorithm PPO --maneuver circular --timesteps 1000000

# SAC ile eÄŸit (kaÃ§an hedef)
python train.py --algorithm SAC --maneuver evasive --timesteps 1000000 --n_envs 8

# TD3 ile eÄŸit (zigzag hedef)
python train.py --algorithm TD3 --maneuver zigzag --timesteps 500000
```

**Output**: `models/` klasÃ¶rÃ¼ne kaydedilir

### 3. Trained Model Evaluation

```bash
# GÃ¶rselleÅŸtirme ile
python evaluate.py models/PPO_circular_*/best_model/best_model.zip --render --n_episodes 10

# Sadece metrikler
python evaluate.py models/SAC_evasive_*/final_model.zip --n_episodes 20
```

## ğŸ“Š Sistem DetaylarÄ±

### FÃ¼ze
- **State**: Pozisyon (x, y), HÄ±z (vx, vy)
- **KontrolÃ¶r**: PID (heading kontrolÃ¼)
- **KÄ±sÄ±tlar**: max_speed=300m/s, max_accel=100m/sÂ²
- **Fizik**: 100 Hz gÃ¼ncelleme (dt=0.01s)

### Hedef
- **HÄ±z**: 150 m/s (fÃ¼zeden yavaÅŸ)
- **Manevralar**:
  - `straight`: Manevra yok
  - `circular`: Sabit dÃ¶nÃ¼ÅŸ hÄ±zÄ±
  - `zigzag`: Periyodik yÃ¶n deÄŸiÅŸimleri
  - `evasive`: FÃ¼zeye tepkisel kaÃ§Ä±ÅŸ

### RL Environment

**Observation (14D)**:
- FÃ¼ze: pozisyon, hÄ±z, PID gains, fuel
- Hedef: pozisyon, hÄ±z
- Relative: mesafe, aÃ§Ä± hatasÄ±

**Action (3D continuous)**:
- `[Î”kp, Î”ki, Î”kd]` âˆˆ [-1, 1]Â³

**Reward**:
- -distance (normalize edilmiÅŸ)
- +hedefe yaklaÅŸma bonusu
- +100 (vurdu)
- -50 (Ä±skaladÄ±)

### Desteklenen Algoritmalar
- **PPO**: On-policy, stabil, iyi baseline
- **SAC**: Off-policy, sample-efficient
- **TD3**: Off-policy, deterministic, robust

## ğŸ“ˆ Beklenen SonuÃ§lar

| Method    | Maneuver  | Hit Rate | Avg Steps |
|-----------|-----------|----------|-----------|
| Sabit PID | Straight  | ~90%     | 120       |
| Sabit PID | Circular  | ~70%     | 180       |
| Sabit PID | Evasive   | ~40%     | 250       |
| RL (PPO)  | Circular  | ~85%     | 150       |
| RL (SAC)  | Evasive   | ~65%     | 200       |

RL ajanlarÄ± zor manevralarda **+10-20% iyileÅŸtirme** gÃ¶stermeli.

## âš™ï¸ KonfigÃ¼rasyon

`config.yaml` dosyasÄ±nÄ± dÃ¼zenle:
- Harita boyutu, vuruÅŸ yarÄ±Ã§apÄ±
- FÃ¼ze/hedef hÄ±zlarÄ±
- PID default deÄŸerleri ve aralÄ±klarÄ±
- Training hyperparameters

## ğŸ¨ GÃ¶rselleÅŸtirme

Pygame renderer gÃ¶sterir:
- FÃ¼ze (cyan) ve hedef (red)
- Trajectory'ler (son 100 nokta)
- VuruÅŸ yarÄ±Ã§apÄ± Ã§emberi
- Real-time info: mesafe, PID gains, fuel, hÄ±z

Kontroller:
- **ESC** veya **Q**: Ã‡Ä±kÄ±ÅŸ

## ğŸ”§ Ä°leri KullanÄ±m

### Paralel Training

```bash
# Daha fazla paralel environment
python train.py --algorithm PPO --n_envs 8 --timesteps 2000000
```

### Hyperparameter Tuning

`train.py` iÃ§inde deÄŸiÅŸtir:
- Learning rate
- Batch size
- Network architecture

### Custom Maneuvers

`src/target.py` iÃ§inde yeni manevra ekle:

```python
elif self.maneuver == 'custom':
    # Kendi manevralarÄ±nÄ±z
    pass
```

## ğŸ› Sorun Giderme

**YavaÅŸ rendering**: `demo.py` veya `evaluate.py` Ã§alÄ±ÅŸtÄ±rÄ±rken `--render` kullanma

**Training converge olmuyor**:
- Timesteps artÄ±r
- `src/environment.py` iÃ§inde reward function ayarla
- FarklÄ± algoritma dene (SAC genelde daha sample-efficient)

**Import errors**: Proje root'undan Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan ve `requirements.txt` yÃ¼klendiÄŸinden emin olun

## ğŸ“š Kaynaklar

1. **Control Systems**: Franklin et al., "Feedback Control of Dynamic Systems"
2. **RL**: Sutton & Barto, "Reinforcement Learning: An Introduction"
3. **PPO**: Schulman et al., "Proximal Policy Optimization Algorithms"
4. **SAC**: Haarnoja et al., "Soft Actor-Critic"
5. **TD3**: Fujimoto et al., "Addressing Function Approximation Error"

---

**Akademik kontrol sistemleri dÃ¶kÃ¼mantasyonu iÃ§in**: `CONTROL_SYSTEM_ARCHITECTURE.md`
