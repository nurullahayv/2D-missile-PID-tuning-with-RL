# 2D Missile PID Tuning with Reinforcement Learning

**Episode-level RL** - RL agent observes full simulation trajectory and learns optimal FIXED PID parameters.

## ðŸŽ¯ AmaÃ§

- **Sistem**: 2D fÃ¼ze (PID kontrollÃ¼) â†’ hareketli hedefi takip et
- **RL GÃ¶revi**: Optimal sabit PID parametrelerini (Kp, Ki, Kd) bul
- **YaklaÅŸÄ±m**: Episode-level RL - TÃ¼m simÃ¼lasyon trajectory'si observation
- **Test**: RecurrentPPO (LSTM) ile trajectory'yi Ã¶ÄŸren

## ðŸ“¦ Stack

- **Gymnasium**: RL environment
- **Pygame**: GÃ¶rselleÅŸtirme
- **PyTorch**: Neural network backend
- **Stable-Baselines3**: RL algorithms (PPO, SAC)
- **SB3-Contrib**: RecurrentPPO (LSTM policy)

## ðŸ—ï¸ YapÄ±

```
src/
  missile.py                    # PID kontrollÃ¼ fÃ¼ze
  target.py                     # Hareketli hedef (4 manevra tipi)
  episodic_fixed_pid_env.py    # Episode-level RL environment â­
  renderer.py                   # Pygame gÃ¶rselleÅŸtirme
train_fixed_pid.py              # RL training (RecurrentPPO) â­
evaluate.py                     # Model evaluation
demo.py                         # Basit demo (RL yok)
kaggle_training_fixed_pid.ipynb # Kaggle GPU training notebook ðŸŽ®
config.yaml                     # KonfigÃ¼rasyon
```

## ðŸš€ Kurulum

```bash
pip install -r requirements.txt
```

**Not:** `sb3-contrib` gerekli (RecurrentPPO iÃ§in)

## ðŸ’» KullanÄ±m

### 1. Demo (RL olmadan, sabit PID)

```bash
# Dairesel manevra yapan hedef
python demo.py --maneuver circular --kp 2.0 --ki 0.1 --kd 0.5

# KaÃ§an hedef
python demo.py --maneuver evasive --kp 3.0 --ki 0.15 --kd 0.8

# DÃ¼z giden hedef (kolay)
python demo.py --maneuver straight --kp 1.5 --ki 0.05 --kd 0.3

# Zigzag yapan hedef
python demo.py --maneuver zigzag --kp 2.5 --ki 0.12 --kd 0.6
```

### 2. RL Training (Episode-level Fixed PID) â­

**En pratik yaklaÅŸÄ±m**: RL ile optimal **sabit** PID parametrelerini bul

```bash
# RecurrentPPO ile circular hedef iÃ§in optimal PID bul
python train_fixed_pid.py --algorithm RecurrentPPO --maneuver circular --timesteps 10000

# Evasive hedef iÃ§in
python train_fixed_pid.py --algorithm RecurrentPPO --maneuver evasive --timesteps 20000

# Standard PPO (LSTM olmadan)
python train_fixed_pid.py --algorithm PPO --maneuver circular --timesteps 10000

# SAC (LSTM olmadan, off-policy)
python train_fixed_pid.py --algorithm SAC --maneuver circular --timesteps 50000
```

**Ã–nemli:**
- 1 timestep = 1 episode (full 500-step simulation)
- RecurrentPPO: LSTM ile trajectory'yi Ã¶ÄŸrenir
- 10K timesteps = 10K episode = ~2-3 hours

**Ã‡Ä±ktÄ±**: Script otomatik olarak optimal PID parametrelerini bulur ve terminale yazdÄ±rÄ±r:
```
Optimal PID Parameters for 'circular' target:
  Kp = 3.245 Â± 0.123
  Ki = 0.187 Â± 0.042
  Kd = 0.712 Â± 0.089

ðŸ’¡ Use these values in demo.py:
   python demo.py --maneuver circular --kp 3.245 --ki 0.187 --kd 0.712
```

**Avantajlar**:
- âœ… GerÃ§ek fÃ¼ze sistemlerine benzer (sabit PID)
- âœ… Yorumlanabilir sonuÃ§lar (somut PID deÄŸerleri)
- âœ… Trajectory observation (tÃ¼m simÃ¼lasyon gÃ¶rÃ¼lÃ¼r)
- âœ… Demo'da test edilebilir

### 3. Kaggle GPU Training (Ã–nerilen!)

**En hÄ±zlÄ± yol: Kaggle'da Ã¼cretsiz GPU ile eÄŸit!**

1. Kaggle'a git: https://www.kaggle.com
2. `kaggle_training_fixed_pid.ipynb` dosyasÄ±nÄ± upload et
3. Settings â†’ Accelerator â†’ **GPU T4** seÃ§
4. "Run All" - 1-2 saatte model hazÄ±r!
5. Optimal PID deÄŸerleri notebook'ta gÃ¶sterilir

**Avantajlar:**
- âœ… Ãœcretsiz GPU (T4/P100)
- âœ… Kurulum yok, direkt Ã§alÄ±ÅŸÄ±r
- âœ… 1-2 saatte eÄŸitim tamamlanÄ±r
- âœ… Optimal PID deÄŸerleri otomatik Ã§Ä±kar

### 4. Trained Model Evaluation

```bash
# GÃ¶rselleÅŸtirme ile
python evaluate.py models/recurrentppo_circular_*/best_model.zip --render --n_episodes 10

# Sadece metrikler
python evaluate.py models/sac_evasive_*/final_model.zip --n_episodes 20
```

## ðŸ“Š Sistem DetaylarÄ±

### FÃ¼ze
- **State**: Pozisyon (x, y), HÄ±z (vx, vy)
- **KontrolÃ¶r**: PID (heading kontrolÃ¼)
- **KÄ±sÄ±tlar**: max_speed=1000m/s, max_accel=1000m/sÂ²
- **Fizik**: 100 Hz gÃ¼ncelleme (dt=0.01s)

### Hedef
- **HÄ±z**: 1000 m/s
- **Manevralar**:
  - `straight`: Manevra yok
  - `circular`: Sabit dÃ¶nÃ¼ÅŸ hÄ±zÄ±
  - `zigzag`: Periyodik yÃ¶n deÄŸiÅŸimleri
  - `evasive`: FÃ¼zeye tepkisel kaÃ§Ä±ÅŸ

### Episode-level RL Environment â­

**Workflow:**
1. RL agent selects [Kp, Ki, Kd] once
2. Environment runs FULL simulation (500 steps)
3. Trajectory is downsampled (every 10 steps â†’ 50 samples)
4. Observation = trajectory features (600D)
5. Reward = episodic (hit, time, trajectory quality)

**Observation (600D)**:
- Downsampled trajectory: 50 samples Ã— 12 features
- Features per sample: [m_x, m_y, m_vx, m_vy, t_x, t_y, t_vx, t_vy, distance, angle_error, closing_velocity, heading_error]

**Action (3D continuous)**:
- `[Kp, Ki, Kd]` - Direkt PID deÄŸerleri
- Kp âˆˆ [0.1, 10000], Ki âˆˆ [0.0, 50], Kd âˆˆ [0.0, 50]
- Episode baÅŸÄ±nda bir kere seÃ§ilir, sonra sabit kalÄ±r

**Reward (Episodic)**:
```python
reward = 0
if hit:
    reward += 100 + time_bonus
else:
    reward -= 50 + distance_penalty

reward -= avg_distance_penalty
reward -= trajectory_smoothness_penalty
reward += closing_velocity_bonus
```

### Desteklenen Algoritmalar
- **RecurrentPPO**: LSTM policy, trajectory sequence Ã¶ÄŸrenir â­
- **PPO**: On-policy, stabil, iyi baseline
- **SAC**: Off-policy, sample-efficient (ama LSTM yok)

## ðŸ“ˆ Beklenen SonuÃ§lar

| Algorithm      | Maneuver  | Hit Rate | Avg Time | Training Time |
|----------------|-----------|----------|----------|---------------|
| RecurrentPPO   | Circular  | ~80%     | 200      | 2-3 hours     |
| RecurrentPPO   | Evasive   | ~60%     | 280      | 3-4 hours     |
| PPO (no LSTM)  | Circular  | ~70%     | 220      | 2 hours       |
| SAC (no LSTM)  | Circular  | ~75%     | 210      | 4-5 hours     |

**RecurrentPPO Ã¶nerilir:** Trajectory sequence'i LSTM ile Ã¶ÄŸrenir.

## âš™ï¸ KonfigÃ¼rasyon

`config.yaml` dosyasÄ±nÄ± dÃ¼zenle:
- Harita boyutu, vuruÅŸ yarÄ±Ã§apÄ±
- FÃ¼ze/hedef hÄ±zlarÄ±
- PID aralÄ±klarÄ± (wide range: Kp up to 10000!)
- Training hyperparameters

## ðŸŽ¨ GÃ¶rselleÅŸtirme

Pygame renderer gÃ¶sterir:
- FÃ¼ze (cyan) ve hedef (red)
- Trajectory'ler (son 100 nokta)
- VuruÅŸ yarÄ±Ã§apÄ± Ã§emberi
- Real-time info: mesafe, PID gains, hÄ±z

Kontroller:
- **ESC** veya **Q**: Ã‡Ä±kÄ±ÅŸ

## ðŸ”§ Ä°leri KullanÄ±m

### Paralel Training

```bash
# Daha fazla paralel environment
python train_fixed_pid.py --algorithm RecurrentPPO --n_envs 8 --timesteps 20000
```

### Hyperparameter Tuning

`train_fixed_pid.py` iÃ§inde deÄŸiÅŸtir:
- Learning rate
- Batch size
- Network architecture
- LSTM hidden size

### Custom Maneuvers

`src/target.py` iÃ§inde yeni manevra ekle:

```python
elif self.maneuver == 'custom':
    # Kendi manevralarÄ±nÄ±z
    pass
```

## ðŸ› Sorun Giderme

**YavaÅŸ training**: Normal! 1 episode = 500 simulation step. RecurrentPPO LSTM overhead ekler.

**LSTM memory error**: `lstm_hidden_size` kÃ¼Ã§Ã¼lt (256 â†’ 128)

**Training converge olmuyor**:
- Timesteps artÄ±r (10K â†’ 20K)
- Reward function ayarla (`episodic_fixed_pid_env.py`)
- FarklÄ± algoritma dene (RecurrentPPO â†’ PPO)

**Import errors**:
```bash
pip install sb3-contrib>=2.0.0
```

## ðŸ“š Kaynaklar

1. **Control Systems**: Franklin et al., "Feedback Control of Dynamic Systems"
2. **RL**: Sutton & Barto, "Reinforcement Learning: An Introduction"
3. **PPO**: Schulman et al., "Proximal Policy Optimization Algorithms"
4. **SAC**: Haarnoja et al., "Soft Actor-Critic"
5. **LSTM**: Hochreiter & Schmidhuber, "Long Short-Term Memory"

---

## ðŸ†š Episode-level vs Step-level RL

| Ã–zellik | Episode-level (Bu Repo) | Step-level |
|---------|-------------------------|------------|
| **Observation** | Full trajectory (600D) | Current state (11D) |
| **Action frequency** | Once per episode | Every step |
| **Training samples** | 1 per episode | 500 per episode |
| **Trajectory** | Explicit | Implicit (LSTM hidden) |
| **Training speed** | Slower (1 sample) | Faster (500 samples) |
| **Information** | Full trajectory | Current state only |
| **Best for PID tuning** | âœ… Yes | Maybe |

**Episode-level daha mantÄ±klÄ± Ã§Ã¼nkÃ¼:**
- TÃ¼m trajectory gÃ¶rÃ¼lÃ¼r (like real PID tuning!)
- Reward episodic (hit, time, quality)
- More interpretable

---

**Akademik kontrol sistemleri dÃ¶kÃ¼mantasyonu iÃ§in**: `CONTROL_SYSTEM_ARCHITECTURE.md`
