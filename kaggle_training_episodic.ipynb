{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Episode-level Fixed PID Training with RecurrentPPO (GPU)\n",
    "\n",
    "**Ama√ß**: RL ile optimal FIXED PID parametrelerini bul (Kp, Ki, Kd)\n",
    "\n",
    "**Yakla≈üƒ±m**: Episode-level RL\n",
    "- 1 timestep = 1 episode (full 500-step simulation)\n",
    "- Action: [Kp, Ki, Kd] - bir kere se√ßilir\n",
    "- Observation: Downsampled trajectory (600D)\n",
    "- Algorithm: RecurrentPPO (LSTM policy)\n",
    "\n",
    "**Kaggle Setup**:\n",
    "1. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2** (√∂nerilen)\n",
    "2. Run All ‚Üí ~1-2 hours\n",
    "3. Optimal PID deƒüerleri sonunda g√∂sterilir\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gymnasium \"numpy<2\" torch stable-baselines3 sb3-contrib tensorboard tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Simulation Classes (Inline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDController:\n",
    "    \"\"\"PID Controller for missile heading\"\"\"\n",
    "    def __init__(self, kp=2.0, ki=0.1, kd=0.5):\n",
    "        self.kp = kp\n",
    "        self.ki = ki\n",
    "        self.kd = kd\n",
    "        self.integral = 0.0\n",
    "        self.prev_error = 0.0\n",
    "\n",
    "    def update(self, error, dt):\n",
    "        self.integral += error * dt\n",
    "        derivative = (error - self.prev_error) / dt if dt > 0 else 0.0\n",
    "        output = self.kp * error + self.ki * self.integral + self.kd * derivative\n",
    "        self.prev_error = error\n",
    "        return output\n",
    "\n",
    "    def reset(self):\n",
    "        self.integral = 0.0\n",
    "        self.prev_error = 0.0\n",
    "\n",
    "\n",
    "class Missile:\n",
    "    \"\"\"2D Missile with PID control\"\"\"\n",
    "    def __init__(self, x=0.0, y=0.0, vx=0.0, vy=0.0,\n",
    "                 max_speed=1000.0, max_accel=1000.0,\n",
    "                 kp=2.0, ki=0.1, kd=0.5):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vx = vx\n",
    "        self.vy = vy\n",
    "        self.max_speed = max_speed\n",
    "        self.max_accel = max_accel\n",
    "        self.pid = PIDController(kp, ki, kd)\n",
    "        self.active = True\n",
    "        self.trajectory = []\n",
    "\n",
    "    @property\n",
    "    def position(self):\n",
    "        return np.array([self.x, self.y])\n",
    "\n",
    "    @property\n",
    "    def velocity(self):\n",
    "        return np.array([self.vx, self.vy])\n",
    "\n",
    "    @property\n",
    "    def speed(self):\n",
    "        return np.linalg.norm(self.velocity)\n",
    "\n",
    "    @property\n",
    "    def heading(self):\n",
    "        return np.arctan2(self.vy, self.vx)\n",
    "\n",
    "    def update(self, target_pos, dt):\n",
    "        if not self.active:\n",
    "            return\n",
    "\n",
    "        # Calculate desired heading\n",
    "        dx = target_pos[0] - self.x\n",
    "        dy = target_pos[1] - self.y\n",
    "        desired_heading = np.arctan2(dy, dx)\n",
    "\n",
    "        # Calculate heading error\n",
    "        current_heading = self.heading\n",
    "        error = desired_heading - current_heading\n",
    "        error = np.arctan2(np.sin(error), np.cos(error))  # Normalize to [-pi, pi]\n",
    "\n",
    "        # PID control\n",
    "        control = self.pid.update(error, dt)\n",
    "        new_heading = current_heading + control * dt\n",
    "\n",
    "        # Calculate desired velocity\n",
    "        desired_vx = self.max_speed * np.cos(new_heading)\n",
    "        desired_vy = self.max_speed * np.sin(new_heading)\n",
    "\n",
    "        # Apply acceleration limits\n",
    "        dvx = desired_vx - self.vx\n",
    "        dvy = desired_vy - self.vy\n",
    "        accel_magnitude = np.sqrt(dvx**2 + dvy**2) / dt if dt > 0 else 0\n",
    "\n",
    "        if accel_magnitude > self.max_accel:\n",
    "            scale = self.max_accel / accel_magnitude\n",
    "            dvx *= scale\n",
    "            dvy *= scale\n",
    "\n",
    "        # Update velocity\n",
    "        self.vx += dvx\n",
    "        self.vy += dvy\n",
    "\n",
    "        # Limit speed\n",
    "        speed = self.speed\n",
    "        if speed > self.max_speed:\n",
    "            self.vx = (self.vx / speed) * self.max_speed\n",
    "            self.vy = (self.vy / speed) * self.max_speed\n",
    "\n",
    "        # Update position\n",
    "        self.x += self.vx * dt\n",
    "        self.y += self.vy * dt\n",
    "\n",
    "        self.trajectory.append((self.x, self.y))\n",
    "\n",
    "\n",
    "class Target:\n",
    "    \"\"\"2D Target with various maneuvers\"\"\"\n",
    "    def __init__(self, x=5000.0, y=5000.0, speed=1000.0, maneuver='circular'):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.speed = speed\n",
    "        self.maneuver = maneuver\n",
    "        self.heading = 0.0\n",
    "        self.maneuver_timer = 0.0\n",
    "        self.trajectory = []\n",
    "\n",
    "    @property\n",
    "    def position(self):\n",
    "        return np.array([self.x, self.y])\n",
    "\n",
    "    @property\n",
    "    def velocity(self):\n",
    "        vx = self.speed * np.cos(self.heading)\n",
    "        vy = self.speed * np.sin(self.heading)\n",
    "        return np.array([vx, vy])\n",
    "\n",
    "    def update(self, dt, missile_pos=None):\n",
    "        self.maneuver_timer += dt\n",
    "\n",
    "        # Apply maneuver\n",
    "        if self.maneuver == 'straight':\n",
    "            pass  # No change in heading\n",
    "\n",
    "        elif self.maneuver == 'circular':\n",
    "            turn_rate = 0.5  # rad/s\n",
    "            self.heading += turn_rate * dt\n",
    "\n",
    "        elif self.maneuver == 'zigzag':\n",
    "            if self.maneuver_timer > 2.0:\n",
    "                self.heading += np.pi / 4\n",
    "                self.maneuver_timer = 0.0\n",
    "\n",
    "        elif self.maneuver == 'evasive' and missile_pos is not None:\n",
    "            dx = self.x - missile_pos[0]\n",
    "            dy = self.y - missile_pos[1]\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            if distance < 2000.0:\n",
    "                escape_heading = np.arctan2(dy, dx)\n",
    "                self.heading = escape_heading\n",
    "\n",
    "        # Update position\n",
    "        vel = self.velocity\n",
    "        self.x += vel[0] * dt\n",
    "        self.y += vel[1] * dt\n",
    "\n",
    "        self.trajectory.append((self.x, self.y))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Simulation classes loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Episode-level Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicFixedPIDEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Episode-level RL environment for finding optimal fixed PID parameters.\n",
    "\n",
    "    Key differences from step-level:\n",
    "    - Action sets PID parameters ONCE at episode start\n",
    "    - Environment runs FULL simulation (500 steps)\n",
    "    - Observation = downsampled trajectory (50 samples √ó 12 features = 600D)\n",
    "    - Reward = episodic (based on full trajectory result)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 60}\n",
    "\n",
    "    def __init__(self, map_size=10000.0, hit_radius=50.0, max_steps=500,\n",
    "                 dt=0.01, target_maneuver='circular',\n",
    "                 missile_speed=1000.0, missile_accel=1000.0,\n",
    "                 target_speed=1000.0, downsample_rate=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Action: Direct PID parameter values (WIDE range for large simulation)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.1, 0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([10000.0, 50.0, 50.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation: Downsampled trajectory (600D)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(600,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Environment parameters\n",
    "        self.map_size = map_size\n",
    "        self.hit_radius = hit_radius\n",
    "        self.max_steps = max_steps\n",
    "        self.dt = dt\n",
    "        self.target_maneuver = target_maneuver\n",
    "        self.missile_speed = missile_speed\n",
    "        self.missile_accel = missile_accel\n",
    "        self.target_speed = target_speed\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "        # State\n",
    "        self.missile = None\n",
    "        self.target = None\n",
    "        self.trajectory = []\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset missile (random initial position and velocity)\n",
    "        missile_x = np.random.uniform(0, 0.2 * self.map_size)\n",
    "        missile_y = np.random.uniform(0.2 * self.map_size, 0.8 * self.map_size)\n",
    "        initial_vx = np.random.uniform(0.8 * self.missile_speed, 0.9 * self.missile_speed)\n",
    "\n",
    "        self.missile = Missile(\n",
    "            x=missile_x, y=missile_y,\n",
    "            vx=initial_vx, vy=0.0,\n",
    "            max_speed=self.missile_speed,\n",
    "            max_accel=self.missile_accel,\n",
    "            kp=2.0, ki=0.1, kd=0.5  # Default (will be overridden)\n",
    "        )\n",
    "\n",
    "        # Reset target (random initial position)\n",
    "        target_x = np.random.uniform(0.6 * self.map_size, 0.9 * self.map_size)\n",
    "        target_y = np.random.uniform(0.3 * self.map_size, 0.7 * self.map_size)\n",
    "        target_heading = np.random.uniform(0, 2 * np.pi)\n",
    "\n",
    "        self.target = Target(\n",
    "            x=target_x, y=target_y,\n",
    "            speed=self.target_speed,\n",
    "            maneuver=self.target_maneuver\n",
    "        )\n",
    "        self.target.heading = target_heading\n",
    "\n",
    "        self.trajectory = []\n",
    "\n",
    "        # Initial observation: zeros (no trajectory yet)\n",
    "        return np.zeros(600, dtype=np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Run FULL simulation with given PID parameters\n",
    "\n",
    "        Args:\n",
    "            action: [Kp, Ki, Kd] PID parameters\n",
    "\n",
    "        Returns:\n",
    "            obs: Downsampled trajectory (600D)\n",
    "            reward: Episodic reward\n",
    "            done: Always True (1 step = 1 episode)\n",
    "            truncated: Always False\n",
    "            info: Episode statistics\n",
    "        \"\"\"\n",
    "        # 1. Set PID parameters from action\n",
    "        Kp = float(action[0])\n",
    "        Ki = float(action[1])\n",
    "        Kd = float(action[2])\n",
    "\n",
    "        self.missile.pid.kp = Kp\n",
    "        self.missile.pid.ki = Ki\n",
    "        self.missile.pid.kd = Kd\n",
    "\n",
    "        # 2. Run FULL simulation (500 steps)\n",
    "        self.trajectory = []\n",
    "        hit = False\n",
    "        hit_time = self.max_steps\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            # Update simulation\n",
    "            self.missile.update(self.target.position, self.dt)\n",
    "            self.target.update(self.dt, missile_pos=self.missile.position)\n",
    "\n",
    "            # Calculate metrics\n",
    "            dx = self.target.x - self.missile.x\n",
    "            dy = self.target.y - self.missile.y\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            # Angle error\n",
    "            desired_heading = np.arctan2(dy, dx)\n",
    "            current_heading = self.missile.heading\n",
    "            angle_error = desired_heading - current_heading\n",
    "            angle_error = np.arctan2(np.sin(angle_error), np.cos(angle_error))\n",
    "\n",
    "            # Closing velocity\n",
    "            missile_vel = self.missile.velocity\n",
    "            target_vel = self.target.velocity\n",
    "            relative_vel = missile_vel - target_vel\n",
    "            range_vec = np.array([dx, dy])\n",
    "            if distance > 0:\n",
    "                closing_velocity = -np.dot(relative_vel, range_vec) / distance\n",
    "            else:\n",
    "                closing_velocity = 0.0\n",
    "\n",
    "            heading_error = angle_error\n",
    "\n",
    "            # Get target velocity components\n",
    "            target_velocity = self.target.velocity\n",
    "\n",
    "            # Record trajectory (12 features)\n",
    "            self.trajectory.append([\n",
    "                self.missile.x,\n",
    "                self.missile.y,\n",
    "                self.missile.vx,\n",
    "                self.missile.vy,\n",
    "                self.target.x,\n",
    "                self.target.y,\n",
    "                target_velocity[0],  # target vx\n",
    "                target_velocity[1],  # target vy\n",
    "                distance,\n",
    "                angle_error,\n",
    "                closing_velocity,\n",
    "                heading_error\n",
    "            ])\n",
    "\n",
    "            # Check hit\n",
    "            if distance < self.hit_radius:\n",
    "                hit = True\n",
    "                hit_time = step\n",
    "                break\n",
    "\n",
    "            # Check out of bounds\n",
    "            if (self.missile.x < -1000 or self.missile.x > self.map_size + 1000 or\n",
    "                self.missile.y < -1000 or self.missile.y > self.map_size + 1000):\n",
    "                break\n",
    "\n",
    "            # Check missile active\n",
    "            if not self.missile.active:\n",
    "                break\n",
    "\n",
    "        # 3. Downsample trajectory\n",
    "        trajectory_array = np.array(self.trajectory)\n",
    "\n",
    "        if len(trajectory_array) > 0:\n",
    "            # Downsample\n",
    "            downsampled = trajectory_array[::self.downsample_rate]\n",
    "\n",
    "            # Ensure exactly 50 samples (pad or trim)\n",
    "            if len(downsampled) < 50:\n",
    "                # Pad with last sample\n",
    "                padding = np.tile(downsampled[-1], (50 - len(downsampled), 1))\n",
    "                downsampled = np.vstack([downsampled, padding])\n",
    "            elif len(downsampled) > 50:\n",
    "                # Trim\n",
    "                downsampled = downsampled[:50]\n",
    "\n",
    "            # Flatten to 600D\n",
    "            obs = downsampled.flatten().astype(np.float32)\n",
    "        else:\n",
    "            # No trajectory (immediate failure)\n",
    "            obs = np.zeros(600, dtype=np.float32)\n",
    "\n",
    "        # 4. Calculate episodic reward\n",
    "        reward = self._calculate_reward(trajectory_array, hit, hit_time)\n",
    "\n",
    "        # 5. Episode info\n",
    "        final_distance = trajectory_array[-1, 8] if len(trajectory_array) > 0 else 10000.0\n",
    "\n",
    "        info = {\n",
    "            'hit': hit,\n",
    "            'hit_time': hit_time,\n",
    "            'final_distance': final_distance,\n",
    "            'trajectory_length': len(trajectory_array),\n",
    "            'pid_kp': Kp,\n",
    "            'pid_ki': Ki,\n",
    "            'pid_kd': Kd\n",
    "        }\n",
    "\n",
    "        # Episode always done after 1 step (full simulation)\n",
    "        return obs, reward, True, False, info\n",
    "\n",
    "    def _calculate_reward(self, trajectory, hit, hit_time):\n",
    "        \"\"\"Calculate episodic reward based on full trajectory\"\"\"\n",
    "        reward = 0.0\n",
    "\n",
    "        if len(trajectory) == 0:\n",
    "            return -100.0\n",
    "\n",
    "        # 1. Hit/Miss\n",
    "        if hit:\n",
    "            reward += 100.0\n",
    "            # Time bonus: Faster intercept is better\n",
    "            time_bonus = (self.max_steps - hit_time) / 10.0\n",
    "            reward += time_bonus\n",
    "        else:\n",
    "            # Miss penalty\n",
    "            reward -= 50.0\n",
    "            # Final distance penalty\n",
    "            final_distance = trajectory[-1, 8]\n",
    "            reward -= final_distance / 1000.0\n",
    "\n",
    "        # 2. Average distance penalty (trajectory quality)\n",
    "        avg_distance = np.mean(trajectory[:, 8])\n",
    "        reward -= avg_distance / 1000.0\n",
    "\n",
    "        # 3. Trajectory smoothness\n",
    "        velocities = trajectory[:, 2:4]\n",
    "        if len(velocities) > 1:\n",
    "            accelerations = np.diff(velocities, axis=0)\n",
    "            jerk = np.diff(accelerations, axis=0)\n",
    "            smoothness_penalty = np.mean(np.linalg.norm(jerk, axis=1))\n",
    "            reward -= smoothness_penalty / 10000.0\n",
    "\n",
    "        # 4. Closing velocity bonus\n",
    "        avg_closing_vel = np.mean(trajectory[:, 10])\n",
    "        if avg_closing_vel > 0:\n",
    "            reward += avg_closing_vel / 1000.0\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "print(\"‚úÖ EpisodicFixedPIDEnv loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "ALGORITHM = 'RecurrentPPO'  # RecurrentPPO with LSTM\n",
    "MANEUVER = 'circular'  # Target maneuver: straight, circular, zigzag, evasive\n",
    "N_ENVS = 8  # Parallel environments (GPU can handle more)\n",
    "TOTAL_TIMESTEPS = 50_000  # Total episodes (50K = ~2 hours on T4)\n",
    "SAVE_FREQ = 5_000  # Checkpoint frequency\n",
    "\n",
    "# Missile/Target parameters\n",
    "MISSILE_SPEED = 1000.0  # m/s\n",
    "MISSILE_ACCEL = 1000.0  # m/s¬≤\n",
    "TARGET_SPEED = 1000.0  # m/s\n",
    "\n",
    "print(f\"\"\"\\n{'='*60}\n",
    "EPISODE-LEVEL FIXED PID TRAINING\n",
    "{'='*60}\n",
    "Algorithm: {ALGORITHM}\n",
    "Target Maneuver: {MANEUVER}\n",
    "Parallel Environments: {N_ENVS}\n",
    "Total Episodes: {TOTAL_TIMESTEPS:,}\n",
    "Missile: {MISSILE_SPEED} m/s, {MISSILE_ACCEL} m/s¬≤\n",
    "Target: {TARGET_SPEED} m/s\n",
    "{'='*60}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(rank):\n",
    "    \"\"\"Create a monitored environment\"\"\"\n",
    "    def _init():\n",
    "        env = EpisodicFixedPIDEnv(\n",
    "            target_maneuver=MANEUVER,\n",
    "            missile_speed=MISSILE_SPEED,\n",
    "            missile_accel=MISSILE_ACCEL,\n",
    "            target_speed=TARGET_SPEED\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "# Create directories\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = f\"models/{ALGORITHM.lower()}_{MANEUVER}_{timestamp}\"\n",
    "log_dir = f\"logs/{ALGORITHM.lower()}_{MANEUVER}_{timestamp}\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Log directory: {log_dir}\\n\")\n",
    "\n",
    "# Create vectorized environments\n",
    "print(\"Creating environments...\")\n",
    "env = DummyVecEnv([make_env(i) for i in range(N_ENVS)])\n",
    "eval_env = DummyVecEnv([make_env(0)])\n",
    "\n",
    "print(f\"‚úÖ Created {N_ENVS} training environments\")\n",
    "print(f\"‚úÖ Created 1 evaluation environment\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initializing {ALGORITHM} model...\\n\")\n",
    "\n",
    "# RecurrentPPO with LSTM policy\n",
    "model = RecurrentPPO(\n",
    "    'MlpLstmPolicy',\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048 // N_ENVS,  # Adjust for vectorized env\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    policy_kwargs={\n",
    "        'lstm_hidden_size': 256,\n",
    "        'n_lstm_layers': 1,\n",
    "        'enable_critic_lstm': True\n",
    "    },\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_dir,\n",
    "    device='cuda'  # Force GPU\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model initialized on {model.device}\")\n",
    "print(f\"\\nüìä Model architecture:\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Callbacks Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=SAVE_FREQ // N_ENVS,\n",
    "    save_path=model_dir,\n",
    "    name_prefix=f\"{ALGORITHM.lower()}_fixed_pid\"\n",
    ")\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=model_dir,\n",
    "    log_path=log_dir,\n",
    "    eval_freq=2_500 // N_ENVS,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Callbacks configured\")\n",
    "print(f\"   - Checkpoints every {SAVE_FREQ:,} steps\")\n",
    "print(f\"   - Evaluation every {2_500:,} steps\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training (GPU Accelerated)\n",
    "\n",
    "**Note**: 1 timestep = 1 episode (full 500-step simulation)\n",
    "\n",
    "**Expected time**: ~1-2 hours for 50K episodes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STARTING TRAINING\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(f\"Monitor training with: tensorboard --logdir {log_dir}\\n\")\n",
    "\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        callback=[checkpoint_callback, eval_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è Training interrupted by user!\")\n",
    "\n",
    "# Save final model\n",
    "final_path = os.path.join(model_dir, f\"{ALGORITHM.lower()}_fixed_pid_final.zip\")\n",
    "model.save(final_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ TRAINING COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final model saved to: {final_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Test Learned PID Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TESTING LEARNED PID PARAMETERS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create test environment\n",
    "test_env = EpisodicFixedPIDEnv(\n",
    "    target_maneuver=MANEUVER,\n",
    "    missile_speed=MISSILE_SPEED,\n",
    "    missile_accel=MISSILE_ACCEL,\n",
    "    target_speed=TARGET_SPEED\n",
    ")\n",
    "\n",
    "n_test_episodes = 20\n",
    "hits = 0\n",
    "total_reward = 0\n",
    "distances = []\n",
    "hit_times = []\n",
    "pid_values = []\n",
    "\n",
    "# RecurrentPPO needs lstm_states\n",
    "lstm_states = None\n",
    "\n",
    "for episode in range(n_test_episodes):\n",
    "    obs, _ = test_env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, deterministic=True)\n",
    "    obs, reward, done, _, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    total_reward += episode_reward\n",
    "    if info['hit']:\n",
    "        hits += 1\n",
    "        hit_times.append(info['hit_time'])\n",
    "\n",
    "    distances.append(info['final_distance'])\n",
    "    pid_values.append([info['pid_kp'], info['pid_ki'], info['pid_kd']])\n",
    "\n",
    "    print(f\"Episode {episode+1:2d}: {'HIT' if info['hit'] else 'MISS':4s} | \"\n",
    "          f\"Time={info['hit_time']:3d} | \"\n",
    "          f\"Dist={info['final_distance']:7.1f}m | \"\n",
    "          f\"Reward={episode_reward:7.1f} | \"\n",
    "          f\"PID=(Kp={info['pid_kp']:.2f}, Ki={info['pid_ki']:.3f}, Kd={info['pid_kd']:.3f})\")\n",
    "\n",
    "# Calculate statistics\n",
    "hit_rate = hits / n_test_episodes * 100\n",
    "avg_reward = total_reward / n_test_episodes\n",
    "avg_hit_time = np.mean(hit_times) if hit_times else 0\n",
    "\n",
    "# Average PID values\n",
    "pid_values = np.array(pid_values)\n",
    "avg_kp = np.mean(pid_values[:, 0])\n",
    "avg_ki = np.mean(pid_values[:, 1])\n",
    "avg_kd = np.mean(pid_values[:, 2])\n",
    "std_kp = np.std(pid_values[:, 0])\n",
    "std_ki = np.std(pid_values[:, 1])\n",
    "std_kd = np.std(pid_values[:, 2])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEST RESULTS ({n_test_episodes} episodes)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Hit Rate: {hit_rate:.1f}%\")\n",
    "print(f\"Average Reward: {avg_reward:.1f}\")\n",
    "print(f\"Average Hit Time: {avg_hit_time:.1f} steps\")\n",
    "print(f\"Average Final Distance: {np.mean(distances):.1f}m\")\n",
    "print(f\"\\nüéØ OPTIMAL PID PARAMETERS for '{MANEUVER}' target:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Kp = {avg_kp:.3f} ¬± {std_kp:.3f}\")\n",
    "print(f\"  Ki = {avg_ki:.3f} ¬± {std_ki:.3f}\")\n",
    "print(f\"  Kd = {avg_kd:.3f} ¬± {std_kd:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"üí° Use these values in demo.py:\")\n",
    "print(f\"   python demo.py --maneuver {MANEUVER} \"\n",
    "      f\"--kp {avg_kp:.3f} --ki {avg_ki:.3f} --kd {avg_kd:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Download Best Model\n",
    "\n",
    "Download `best_model.zip` from the model directory to use locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved models\n",
    "print(f\"\\nüìÅ Saved models in {model_dir}:\")\n",
    "!ls -lh {model_dir}\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Download 'best_model.zip' to use locally.\")\n",
    "print(f\"\\nüéØ Optimal PID for '{MANEUVER}' target:\")\n",
    "print(f\"   Kp = {avg_kp:.3f}\")\n",
    "print(f\"   Ki = {avg_ki:.3f}\")\n",
    "print(f\"   Kd = {avg_kd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "### Episode-level RL Approach:\n",
    "- **Action once**: PID parameters [Kp, Ki, Kd] selected at episode start\n",
    "- **Full simulation**: 500 steps run with fixed PID\n",
    "- **Trajectory observation**: 600D (50 samples √ó 12 features)\n",
    "- **Episodic reward**: Based on hit/miss, time, trajectory quality\n",
    "\n",
    "### Why RecurrentPPO?\n",
    "- LSTM policy can learn from trajectory sequences\n",
    "- Better for temporal patterns in simulation data\n",
    "- More stable than step-level RL for PID tuning\n",
    "\n",
    "### Training Time:\n",
    "- 50K episodes ‚âà 1-2 hours on T4 GPU\n",
    "- Can increase to 100K for better convergence\n",
    "- Hit rate typically reaches 70-80% for circular target\n",
    "\n",
    "### Different Maneuvers:\n",
    "- `straight`: Easiest (80-90% hit rate)\n",
    "- `circular`: Medium (70-80% hit rate)\n",
    "- `zigzag`: Medium-hard (60-70% hit rate)\n",
    "- `evasive`: Hardest (50-60% hit rate)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
